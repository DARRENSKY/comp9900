belongTB={
  "outline": [
    "overview outline",
    "neuroanatomy outline",
    "convolutional network outline",
    "evolutionary art outline",
    "course review outline"
  ],
  "http www cse unsw edu au cs9444": [
    "course web page"
  ],
  "alan blair": [
    "lecturer in charge"
  ],
  "blair cse unsw edu au": [
    "lecturer in charge"
  ],
  "k17 412c": [
    "lecturer in charge"
  ],
  "9385 7131": [
    "lecturer in charge"
  ],
  "you must keep up lecture either attending in person or watching recording student enrolled in web stream welcome attend in person if space is available": [
    "lecture"
  ],
  "well attending lecture consider doing these thing": [
    "lecture"
  ],
  "review lecture material after lecture": [
    "lecture",
    "well attending lecture consider doing these thing"
  ],
  "discus material fellow student if possible": [
    "lecture",
    "well attending lecture consider doing these thing"
  ],
  "read up topic covered in each lecture": [
    "lecture",
    "well attending lecture consider doing these thing"
  ],
  "complete relevant assignment exercise if any": [
    "lecture",
    "well attending lecture consider doing these thing"
  ],
  "explore topic writing running ymy own program": [
    "lecture",
    "well attending lecture consider doing these thing"
  ],
  "attend consultation session ask question": [
    "lecture",
    "well attending lecture consider doing these thing"
  ],
  "mon 3 4 string bras": [
    "lab"
  ],
  "wed 4 5 flute oboe": [
    "lab"
  ],
  "thu 11 12 clavier organ": [
    "lab"
  ],
  "lecture week 1 9 11 13": [
    "lecture or lab schedule"
  ],
  "lab optional tentative week 2 9 11 12": [
    "lecture or lab schedule"
  ],
  "textbook thi course is": [
    "textbook"
  ],
  "deep learning": [
    "textbook"
  ],
  "ian goodfellow yoshua bengio aaron courville": [
    "textbook"
  ],
  "mit pres": [
    "textbook"
  ],
  "http www deeplearningbook org": [
    "textbook"
  ],
  "http mitpres mit edu book deep learning": [
    "textbook"
  ],
  "course will assume knowledge following mathematical topic": [
    "assumed knowledge"
  ],
  "linear algebra": [
    "assumed knowledge"
  ],
  "probability": [
    "assumed knowledge"
  ],
  "calculu chain rule": [
    "assumed knowledge"
  ],
  "student should study relevant section textbook shown inbracket if necessary try revise these topic their own during first few week course": [
    "assumed knowledge"
  ],
  "neuroanatomy": [
    "planned topic"
  ],
  "perceptron backpropagation": [
    "planned topic"
  ],
  "hidden unit dynamic": [
    "planned topic",
    "recurrent network outline",
    "recurrent network"
  ],
  "convolutional network": [
    "planned topic"
  ],
  "recurrent network": [
    "planned topic"
  ],
  "long short term memory": [
    "planned topic",
    "recurrent network outline",
    "recurrent network"
  ],
  "autoencoder": [
    "planned topic"
  ],
  "deep reinforcement learning": [
    "planned topic"
  ],
  "unsupervised learning": [
    "planned topic",
    "type learning"
  ],
  "hopfield network restricted boltzmann machine": [
    "planned topic"
  ],
  "generative model": [
    "planned topic",
    "boltzmann machine outline",
    "autoencoder outline"
  ],
  "hopfield network": [
    "planned topic",
    "boltzmann machine outline"
  ],
  "assessment will consist": [
    "assessment"
  ],
  "assignment 40": [
    "assessment"
  ],
  "written exam 60": [
    "assessment"
  ],
  "in order pas course you must score": [
    "assessment"
  ],
  "at least 16 40 assignment": [
    "assessment"
  ],
  "at least 24 60 written exam": [
    "assessment"
  ],
  "combined mark at least 50 100": [
    "assessment"
  ],
  "we planning have 4 assignment during session": [
    "assignment"
  ],
  "assignment may involve example": [
    "assignment"
  ],
  "using code written in neural network simulation package tensorflow theano kera": [
    "assignment"
  ],
  "writing ymy own code": [
    "assignment"
  ],
  "running experiment analysing result": [
    "assignment"
  ],
  "further detail will be provided course web site": [
    "assignment"
  ],
  "plagiarism is taken seriously unsw cse treated academic misconduct all work submitted assessment must be ymy own work": [
    "plagiarism"
  ],
  "an individual assignment collaborative work in form think tanking is encouraged but student not allowed derive code together group during such discussion in case group assignment code must not be obtained from outside group": [
    "plagiarism"
  ],
  "plagiarism detection software may be used submitted work": [
    "plagiarism"
  ],
  "academic integrity plagiarism": [
    "plagiarism"
  ],
  "http student unsw edu au plagiarism": [
    "plagiarism"
  ],
  "comp3411 9414 artificial intelligence": [
    "related course"
  ],
  "comp9417 machine learning datum mining": [
    "related course"
  ],
  "comp9418 advanced topic in statistical machine learning": [
    "related course"
  ],
  "comp4418 knowledge representation reasoning": [
    "related course"
  ],
  "comp3431 robotic software architecture": [
    "related course"
  ],
  "comp9517 machine vision": [
    "related course"
  ],
  "4th year thesi topic": [
    "related course"
  ],
  "massively parallel distributed processor made up simple processing unit": [
    "neural network"
  ],
  "knowledge acquired from environment through learning proces": [
    "neural network"
  ],
  "knowledge stored in form synaptic weight": [
    "neural network"
  ],
  "biologically inspired": [
    "why neural network"
  ],
  "good learning property": [
    "why neural network"
  ],
  "continuou nonlinear": [
    "why neural network"
  ],
  "well adapted certain task": [
    "why neural network"
  ],
  "fault tolerant": [
    "why neural network"
  ],
  "graceful degradation": [
    "why neural network"
  ],
  "380bc plato rationalism innatenes": [
    "theory intelligence"
  ],
  "330bc aristotle empricism experience": [
    "theory intelligence"
  ],
  "1641 descarte mind body dualism": [
    "theory intelligence"
  ],
  "1781 kant critique pure reason": [
    "theory intelligence"
  ],
  "1899 sigmund freud psychology": [
    "theory intelligence"
  ],
  "1953 b f skinner behaviourism": [
    "theory intelligence"
  ],
  "1642 blaise pascal mechanical adding machine": [
    "artificial intelligence origin"
  ],
  "1694 gottfried leibniz mechanical calculator": [
    "artificial intelligence origin"
  ],
  "1769 wolfgang von kempelen mechanical turk": [
    "artificial intelligence origin"
  ],
  "1837 charle babbage ada lovelace difference engine": [
    "artificial intelligence origin"
  ],
  "1848 george boole calculu logic": [
    "artificial intelligence origin"
  ],
  "1879 gottlob frege predicate logic": [
    "artificial intelligence origin"
  ],
  "1950 turing test": [
    "artificial intelligence origin"
  ],
  "1956 dartmouth conference": [
    "artificial intelligence origin"
  ],
  "1943 mcculloch pitt neuron model": [
    "neural network origin"
  ],
  "1948 norbert wiener cybernetic": [
    "neural network origin"
  ],
  "1948 alan turing b type network": [
    "neural network origin"
  ],
  "1955 oliver selfridge pattern recognition": [
    "neural network origin"
  ],
  "1962 hubel wiesel visual cortex": [
    "neural network origin"
  ],
  "1962 frank rosenblatt perceptron": [
    "neural network origin"
  ],
  "1956 newell simon logic theorist": [
    "serial symbolic ai"
  ],
  "1959 john mccarthy lisp": [
    "serial symbolic ai"
  ],
  "1959 arther samuel checker": [
    "serial symbolic ai"
  ],
  "1965 joseph weizenbaum eliza": [
    "serial symbolic ai"
  ],
  "1967 edward feigenbaum dendral": [
    "serial symbolic ai"
  ],
  "1969 minsky papert published perceptron emphasizing limitation neural model lobbied agency cease funding neural network research": [
    "neural network dark age"
  ],
  "from 1969 1985 there wa very little work in neural network or machine learning": [
    "neural network dark age"
  ],
  "few exception e g stephen grossberg teuvo kohonen som paul werbo": [
    "neural network dark age"
  ],
  "combinatorial explosion in search space": [
    "some commercial succes but ran into difficulty",
    "knowledge based system"
  ],
  "difficulty formalising everyday knowledge well expert": [
    "some commercial succes but ran into difficulty"
  ],
  "1970 early 1980 ai research focused symbolic processing expert system": [
    "knowledge based system"
  ],
  "some commercial succes but ran into difficulty": [
    "knowledge based system"
  ],
  "difficulty formalising everyday knowledge well expert knowledge": [
    "knowledge based system"
  ],
  "1986 rumelhart hinton william multus layer backprop": [
    "neural network renaissance"
  ],
  "1989 dean pomerleau alvinn": [
    "neural network renaissance"
  ],
  "late 1980 renewed enthusiasm hype": [
    "neural network renaissance"
  ],
  "1990 more principled approach": [
    "neural network renaissance"
  ],
  "2000 svm bayesian model became more popular": [
    "neural network renaissance"
  ],
  "2010 deep learning network gpu s": [
    "neural network renaissance"
  ],
  "2020 spiking network": [
    "neural network renaissance"
  ],
  "classification": [
    "image processing",
    "application deep learning"
  ],
  "segmentation": [
    "image processing",
    "application deep learning"
  ],
  "translation": [
    "language processing",
    "application deep learning"
  ],
  "semantic disambiguation": [
    "language processing",
    "application deep learning"
  ],
  "sentiment analysi": [
    "language processing",
    "application deep learning"
  ],
  "automatic captioning": [
    "combining image tex",
    "application deep learning"
  ],
  "alphago": [
    "game playing",
    "application deep learning"
  ],
  "deep q learning": [
    "game playing",
    "application deep learning"
  ],
  "image processing": [
    "application deep learning"
  ],
  "language processing": [
    "application deep learning"
  ],
  "combining image tex": [
    "application deep learning"
  ],
  "game playing": [
    "application deep learning",
    "two layer nn application"
  ],
  "two perspective history deep learning": [
    "history deep learning"
  ],
  "viewpoint 1 focusing recent work after 2012": [
    "history deep learning"
  ],
  "http www cs toronto edu hinton absp naturedeepreview pdf": [
    "history deep learning"
  ],
  "viewpoint 2 focusing earlier work before 2012": [
    "history deep learning"
  ],
  "http person idsium ch juergen deep learning overview html": [
    "history deep learning"
  ],
  "brain": [
    "central nervou system",
    "neuroanatomy"
  ],
  "spinal cord": [
    "central nervou system",
    "neuroanatomy"
  ],
  "somatic nervou system": [
    "peripheral nervou system",
    "neuroanatomy"
  ],
  "autonomic nervou system": [
    "peripheral nervou system",
    "neuroanatomy"
  ],
  "enteric nervou system": [
    "peripheral nervou system",
    "neuroanatomy"
  ],
  "central nervou system": [
    "neuroanatomy"
  ],
  "peripheral nervou system": [
    "neuroanatomy"
  ],
  "cortex from latin word bark tree": [
    "cerebral cortex"
  ],
  "right left side connected corpu callosum": [
    "cerebral cortex"
  ],
  "function thought voluntary movement language reasoning perception": [
    "cerebral cortex"
  ],
  "general term area brain between thalamu spinal cord": [
    "brain stem"
  ],
  "include medulla pon tectum reticular formation tegmentum": [
    "brain stem"
  ],
  "function breathing heart rate blood pressure other": [
    "brain stem"
  ],
  "from latin word little brain": [
    "cerebellum"
  ],
  "function movement balance posture": [
    "cerebellum"
  ],
  "function vision audition eye movement body movement": [
    "midbrain"
  ],
  "receife sensory information relay it cerebral cortex": [
    "thalamu"
  ],
  "also relay information from cerebral cortex other area brain spinal cord": [
    "thalamu"
  ],
  "function sensory integration motor integration": [
    "thalamu"
  ],
  "composed several different area at base brain": [
    "hypothalamu"
  ],
  "size pea 1 300 total brain weight": [
    "hypothalamu"
  ],
  "function body temperature emotion hunger thirst circadian rhythm": [
    "hypothalamu"
  ],
  "group structure including amygdala hippocampu mammillary body cingulate gyru": [
    "limbic system"
  ],
  "important controlling emotional response given situation": [
    "limbic system"
  ],
  "hippocampu also important memory": [
    "limbic system"
  ],
  "function emotional behavimy": [
    "limbic system"
  ],
  "neuron surrounded cell membrane": [
    "neuron similar other cell in body in some way such",
    "neuron body cell"
  ],
  "neuron have nucleu that contain gene dna": [
    "neuron similar other cell in body in some way such",
    "neuron body cell"
  ],
  "neuron carry out basic cellular process like protein synthesi": [
    "neuron similar other cell in body in some way such",
    "neuron body cell"
  ],
  "body is made up billion cell cell nervoussystem calledneuron specialized carry message through an electrochemical proces": [
    "neuron body cell"
  ],
  "human brain ha 100 billion neuron similar numberof support cell called glium": [
    "neuron body cell"
  ],
  "neuron similar other cell in body in some way such": [
    "neuron body cell"
  ],
  "unipolar bipolar neuron have only one dendrite": [
    "most neuron have only one axon but number dendrite can vary widely",
    "neuron versu body cell"
  ],
  "purkinje neuron can have up 100 000 dendrite": [
    "most neuron have only one axon but number dendrite can vary widely",
    "neuron versu body cell"
  ],
  "neuron have specialized extension called dendrite axon dendrite bring information cell body while axon take information away from cell body": [
    "neuron versu body cell"
  ],
  "axon one neuron can connect dendrite another neuron through an electrochemical junction called synapse": [
    "neuron versu body cell"
  ],
  "most neuron have only one axon but number dendrite can vary widely": [
    "neuron versu body cell"
  ],
  "dendrite typically les than millimetre in length": [
    "axon dendrite"
  ],
  "axon can vary in length from les than millimetre more than metre motor neuron": [
    "axon dendrite"
  ],
  "long axon sometime surrounded myelinated sheath which prevent electrical signal from dispersing allows it travel faster up 100 m": [
    "axon dendrite"
  ],
  "electrical pulse reach endbulb cause release neurotransmitter molecule from little packet vesicle through synaptic membrane": [
    "synapse ion channel"
  ],
  "transmitter then diffuse through synaptic cleft other side": [
    "synapse ion channel"
  ],
  "when neurotransmitter reach post synaptic membrane it cause change in polarisation membrane": [
    "synapse ion channel"
  ],
  "change in potential can be excitatiory moving potential toward threshold orinhibitory moving it away from threshold": [
    "synapse ion channel"
  ],
  "human brain ha 100 billion neuron an average 10 000 synapse each": [
    "big picture"
  ],
  "latency is 3 6 millisecond": [
    "big picture"
  ],
  "therefore at most few hundred step in any mental computation but massively parallel": [
    "big picture"
  ],
  "cell in visual cortex respond line at different angle": [
    "hubel weisel visual cortex"
  ],
  "cell in v2 respond more sophisticated visual feature": [
    "hubel weisel visual cortex"
  ],
  "convolutional neural network inspired thi neuroanatomy": [
    "hubel weisel visual cortex"
  ],
  "cnn can now be simulated massive parallelism using gpu s": [
    "hubel weisel visual cortex"
  ],
  "suppose we want classify an image bird sunset dog cat etc": [
    "convolutional network"
  ],
  "if we can identify feature such feather eye or beak which provide useful information in one part image then those feature likely also be relevant in another part image": [
    "convolutional network"
  ],
  "we can exploit thi regularity using convolution layer which apply same weight different part image": [
    "convolutional network"
  ],
  "first layer": [
    "convolutional filter"
  ],
  "second layer": [
    "convolutional filter"
  ],
  "third layer": [
    "convolutional filter"
  ],
  "can unroll recurrent architecture into an equivalent feedforward architecture shared weight": [
    "recurrent neural network",
    "rnn"
  ],
  "useful processing language or other temporal sequence": [
    "recurrent neural network",
    "rnn"
  ],
  "biological neuron spike in different pattern quiescent persistent sporadic": [
    "spiking neuron"
  ],
  "spike timing might carry important information": [
    "spiking neuron"
  ],
  "most nn model ignore timing information but somework ha been done spiking network model": [
    "spiking neuron"
  ],
  "in future special hardware might lead revolution spiking network similar what gpu provided cnn s": [
    "spiking neuron"
  ],
  "neuron biological artificial": [
    "perceptron outline"
  ],
  "perceptron learning": [
    "perceptron outline"
  ],
  "linear separability": [
    "perceptron outline"
  ],
  "multus layer network": [
    "perceptron outline",
    "backpropagation outline"
  ],
  "brain is made up neuron nerve cell which have": [
    "biological neuron"
  ],
  "cell body soma": [
    "biological neuron"
  ],
  "dendrite input": [
    "biological neuron"
  ],
  "an axon output": [
    "biological neuron"
  ],
  "synapse connection between cell": [
    "biological neuron"
  ],
  "synapse can be exitatory or inhibitory may change over time": [
    "biological neuron"
  ],
  "when input reach some threshhold anaction potential electrical pulse is sent along axon output": [
    "biological neuron"
  ],
  "artificial neural network made up node which have": [
    "artificial neural network"
  ],
  "input edge each some weight": [
    "artificial neural network"
  ],
  "output edge weight": [
    "artificial neural network"
  ],
  "an activation level function input": [
    "artificial neural network"
  ],
  "weight can be positive or negative may change over time learning input function is weighted sum activation level input activation level is non linear transfer function g thi input": [
    "artificial neural network"
  ],
  "activationi g si g wijxj": [
    "artificial neural network"
  ],
  "some node input sensing some output action": [
    "artificial neural network"
  ],
  "originally discontinuou step function wa used t he transfer function": [
    "transfer function"
  ],
  "g 1 if s 0 0 if s 0": [
    "transfer function"
  ],
  "later other transfer function were introduced which continuou andsmooth": [
    "transfer function"
  ],
  "question what kind function can perceptron compute": [
    "linear separability"
  ],
  "answer linearly separable function": [
    "linear separability"
  ],
  "example linearly separable function": [
    "linear separability"
  ],
  "w1 w2 1 0 w0 1 5": [
    "linear separability"
  ],
  "or w1 w2 1 0 w0 0 5": [
    "linear separability"
  ],
  "nor w1 w2 1 0 w0 0 5": [
    "linear separability"
  ],
  "w1x1 w2x2 w0 0": [
    "perceptron learning example"
  ],
  "learning rate 0 1": [
    "perceptron learning example"
  ],
  "begin random weight": [
    "perceptron learning example"
  ],
  "w1 0 2": [
    "perceptron learning example"
  ],
  "w2 0 0": [
    "perceptron learning example"
  ],
  "w0 0 1": [
    "perceptron learning example"
  ],
  "0 2x1 0 0x 0 1 0": [
    "training step 1"
  ],
  "w1 w1 x1 0 1": [
    "training step 1",
    "training step 3"
  ],
  "w2 w2 x2 0 1": [
    "training step 1"
  ],
  "w0 w0 0 2": [
    "training step 1",
    "training step 3"
  ],
  "0 1x1 0 1x2 0 2 0": [
    "training step 2"
  ],
  "w1 w1 x1 0 3": [
    "training step 2"
  ],
  "w2 w2 x2 0 0": [
    "training step 2"
  ],
  "w0 w0 0 1": [
    "training step 2"
  ],
  "0 3x1 0 0x2 0 1 0": [
    "training step 3"
  ],
  "3rd point correctly classified": [
    "training step 3"
  ],
  "so no change": [
    "training step 3"
  ],
  "4th point": [
    "training step 3"
  ],
  "w2 w2 x2 0 2": [
    "training step 3"
  ],
  "0 1x1 0 2x2 0 2 0": [
    "training step 3"
  ],
  "eventually all datum will be correctly classified provided it is linearly separable": [
    "final outcome"
  ],
  "problem many useful function not linearly separable e g xor": [
    "limitation perceptron"
  ],
  "possible solution": [
    "limitation perceptron"
  ],
  "x1 xor x2 can be written x1 x2 nor x1 nor x2": [
    "limitation perceptron"
  ],
  "recall that or nor can be implemented perceptron": [
    "limitation perceptron"
  ],
  "problem how can we train it learn new function credit assignment": [
    "multus layer neural network",
    "multus layer network"
  ],
  "in 1969 minsky papert published book highlighting limitation perceptron lobbied variou funding agency redirect funding away from neural network research preferring instead logic based method such expert system": [
    "historical context"
  ],
  "it wa known far back 1960 that any given logical function could be implemented in 2 layer neural network step function activation but question how learn weight amultus layer neural network based training example remained an open problem solution which we describe in next section wa found in 1976 paul werbo but did not become widely known until it wa rediscovered in 1986 rumelhart hinton william": [
    "historical context"
  ],
  "supervised learning": [
    "backpropagation outline",
    "type learning"
  ],
  "ockham razor 5 2": [
    "backpropagation outline"
  ],
  "gradient descent 4 3 6 5 2": [
    "backpropagation outline"
  ],
  "we have training set test set each consisting set item each item number input attribute target value specified": [
    "supervised learning"
  ],
  "aim is predict target value based input attribute": [
    "supervised learning"
  ],
  "agent is presented input target output each item in training set it must then predict output each item in test set": [
    "supervised learning"
  ],
  "variou learning paradigm available": [
    "supervised learning"
  ],
  "decision tree": [
    "supervised learning",
    "supervised learning",
    "variou learning paradigm available"
  ],
  "neural network": [
    "supervised learning",
    "supervised learning",
    "variou learning paradigm available"
  ],
  "svm": [
    "supervised learning",
    "variou learning paradigm available"
  ],
  "other": [
    "supervised learning",
    "variou learning paradigm available"
  ],
  "support vector machine": [
    "supervised learning"
  ],
  "agent is not presented target output but is given reward": [
    "reinforcement learning"
  ],
  "agent is only presented input themself aim find structure in these input": [
    "unsupervised learning"
  ],
  "agent is presented example input their target output": [
    "type learning"
  ],
  "reinforcement learning": [
    "type learning"
  ],
  "agent is not presented target output but is given reward signal which it aim maximize": [
    "type learning"
  ],
  "agent is only presented input themself aim tofind structure in these input": [
    "type learning"
  ],
  "framework decision tree neural network svm etc": [
    "supervised learning issue"
  ],
  "representation input output": [
    "supervised learning issue"
  ],
  "pre processing post processing": [
    "supervised learning issue"
  ],
  "training method perceptron learning backpropagation etc": [
    "supervised learning issue"
  ],
  "generalization avoid over fitting": [
    "supervised learning issue"
  ],
  "evaluation separate training t esting set": [
    "supervised learning issue"
  ],
  "which curve gife best fit these datum": [
    "curve fitting"
  ],
  "which curve gife best fit these datum straight line": [
    "curve fitting"
  ],
  "which curve gife best fit these datum parabola": [
    "curve fitting"
  ],
  "which curve gife best fit these datum 4th order polynomial": [
    "curve fitting"
  ],
  "which curve gife best fit these datum something else": [
    "curve fitting"
  ],
  "most likely hypothesi is simplest one consistent datum": [
    "ockham razor"
  ],
  "inadequate good compromise over fitting": [
    "ockham razor"
  ],
  "since there can be noise in measurement in practice need make tradeoff between simplicity hypothesi how well it fit datum": [
    "ockham razor"
  ],
  "predicted buchanan vote county": [
    "outlier"
  ],
  "faculty washington edu mtbrett": [
    "outlier"
  ],
  "normally number input output unit fixed but we can choose number hidden unit": [
    "two layer neural network"
  ],
  "thi toy problem there is only training set there is no validationor test set so we don t worry overfitting": [
    "xor problem"
  ],
  "xor datum cannot be learned perceptron but can beachieved using 2 layer network two hidden unit": [
    "xor problem"
  ],
  "u1 b1 w11x1 w12x2": [
    "neural network equation"
  ],
  "y g u1": [
    "neural network equation"
  ],
  "c v1y1 v2y2": [
    "neural network equation"
  ],
  "z g": [
    "neural network equation"
  ],
  "e 1 2 z t 2": [
    "neural network equation",
    "gradient descent"
  ],
  "we sometime use w shorthand any trainable weight c v1 v2 b1 b2 w11 w21 w12 w22": [
    "neural network equation"
  ],
  "we define an error function e be half sum over all input pattern square difference between actual output desired output": [
    "nn training cost minimization"
  ],
  "e 1 2 z t": [
    "nn training cost minimization",
    "error function"
  ],
  "if we think e height it define an error landscape weightspace aim is fnd set weight which e is very low": [
    "nn training cost minimization"
  ],
  "problem because step function landscape will not be smooth but will instead consist almost entirely at local region shoulder occasional discontinuou jump": [
    "local search in weight space"
  ],
  "recall that error function e is half sum over all input patternsof square difference between actual output desired output": [
    "gradient descent"
  ],
  "aim is find set weight which e is very low if function involved smooth we can use multus variable calculu adjust weight in such way take u in steepest downhill direction": [
    "gradient descent"
  ],
  "w w e w": [
    "gradient descent"
  ],
  "parameter is called learning rate": [
    "gradient descent"
  ],
  "medical dignosi": [
    "two layer nn application"
  ],
  "autonomou driving": [
    "two layer nn application"
  ],
  "credit card fraud detection": [
    "two layer nn application"
  ],
  "handwriting recognition": [
    "two layer nn application",
    "processing temporal sequence"
  ],
  "financial prediction": [
    "two layer nn application"
  ],
  "limit number hidden node or connection": [
    "three different way prevent overfitting",
    "training tip",
    "training tip"
  ],
  "limit training time using validation set": [
    "three different way prevent overfitting",
    "training tip",
    "training tip"
  ],
  "weight decay": [
    "three different way prevent overfitting",
    "training tip",
    "training tip",
    "variation backprop"
  ],
  "problem weight blow up inhibit further learning": [
    "three different way prevent overfitting",
    "weight decay",
    "variation backprop"
  ],
  "mathematical theory baye rule": [
    "three different way prevent overfitting",
    "weight decay",
    "variation backprop"
  ],
  "solution add weight decay term error function": [
    "three different way prevent overfitting",
    "weight decay",
    "variation backprop"
  ],
  "re scale input output be in range 0 1 or 1 1": [
    "training tip"
  ],
  "replace missing value mean value that attribute": [
    "training tip"
  ],
  "initialize weight very small random value": [
    "training tip"
  ],
  "line or batch learning": [
    "training tip"
  ],
  "three different way prevent overfitting": [
    "training tip"
  ],
  "adjust learning rate momentum suit particular task": [
    "training tip"
  ],
  "note x axi could also be number hidden node or connection": [
    "overfitting in neural network"
  ],
  "8 32 range finder input retina": [
    "later version included sonar range finder",
    "alvinn",
    "alvinn"
  ],
  "29 hidden unit": [
    "later version included sonar range finder",
    "alvinn",
    "alvinn"
  ],
  "45 output unit": [
    "later version included sonar range finder",
    "alvinn",
    "alvinn"
  ],
  "additional transformed training item cover emergency situation": [
    "supervised learning from human action behavioral cloning",
    "alvinn",
    "alvinn"
  ],
  "autonomou land vehicle in neural network": [
    "alvinn"
  ],
  "later version included sonar range finder": [
    "alvinn"
  ],
  "supervised learning from human action behavioral cloning": [
    "alvinn"
  ],
  "drove autonomously from coast coast": [
    "alvinn"
  ],
  "neural network biologically inspired": [
    "summary"
  ],
  "multus layer neural network can learn non linearly separable function": [
    "summary"
  ],
  "backpropagation is effective widely used": [
    "summary"
  ],
  "probability 3 1 3 6 3 9 3 3 10": [
    "variation backprop outline"
  ],
  "cros entropy 5 5": [
    "variation backprop outline"
  ],
  "baye rule 3 11": [
    "variation backprop outline"
  ],
  "weight decay 5 2 2": [
    "variation backprop outline"
  ],
  "momentum 8 3": [
    "variation backprop outline"
  ],
  "begin set sample space e g 6 possible roll die": [
    "probability"
  ],
  "is sample point possible world atomic event": [
    "probability"
  ],
  "probability space or probability model is sample space": [
    "probability"
  ],
  "an assignment p every t": [
    "probability"
  ],
  "0 p 1": [
    "probability"
  ],
  "p 1": [
    "probability"
  ],
  "e g p 1 p 2 p 3 p 4 p 5 p 6 1 6": [
    "probability"
  ],
  "an event is any subset": [
    "probability"
  ],
  "p p": [
    "probability"
  ],
  "e g p dieroll 4 p 1 p 2 p 3 1 6 1 6 1 6 1 2": [
    "probability"
  ],
  "random variable r v is function from sample point some range e g real or b oolean": [
    "random variable"
  ],
  "example odd 3 true": [
    "random variable"
  ],
  "p induce aprobability distribution any r v x": [
    "random variable"
  ],
  "p x xi p": [
    "random variable"
  ],
  "e g p odd true p 1 p 3 p 5 1 6 1 6 1 6 1 2": [
    "random variable"
  ],
  "logically related event must have related probability example p b p p b p b": [
    "probability logic"
  ],
  "e g p x x u 18 26 x uniform density between 18 26": [
    "probability continuou variable"
  ],
  "here p is density integrate 1": [
    "probability continuou variable"
  ],
  "p x 20 5 0 125 really mean": [
    "probability continuou variable"
  ],
  "lim p 20 5 x 20 5 dx dx 0 125": [
    "probability continuou variable"
  ],
  "problem least square error function unsuitable classification where target 0 or 1": [
    "cros entropy",
    "variation backprop"
  ],
  "mathematical theory maximum likelihood": [
    "cros entropy",
    "variation backprop"
  ],
  "solution replace cros entropy error function": [
    "cros entropy",
    "variation backprop"
  ],
  "classification task target t is either 0 or 1 so better use": [
    "cros entropy"
  ],
  "e t log z 1 t log 1 z": [
    "cros entropy"
  ],
  "thi can be justified mathematically work well in practice especially when negative example vastly outweigh positive one": [
    "cros entropy"
  ],
  "it also make backprop computation simpler": [
    "cros entropy"
  ],
  "problem weight oscillate in rain gutter": [
    "momentum",
    "variation backprop"
  ],
  "solution weighted average gradient over time": [
    "momentum",
    "variation backprop"
  ],
  "cros entropy": [
    "variation backprop"
  ],
  "momentum": [
    "variation backprop"
  ],
  "h is clas hypothesis": [
    "maximum likelihood",
    "baye rule in machine learning"
  ],
  "p d h probability datum d being generated under hypothesi h h": [
    "maximum likelihood",
    "baye rule in machine learning"
  ],
  "logp d h is called likelihood": [
    "maximum likelihood"
  ],
  "ml principle choose h h which maximize likelihood": [
    "maximum likelihood"
  ],
  "i e maximize p d h or maximize log p d h": [
    "maximum likelihood"
  ],
  "suppose datum generated linear function h plu gaussian noise withstandard deviation": [
    "derivation least square"
  ],
  "question suppose we have test type cancer which occur in 1 patient test ha sensitivity 98 specificity 97 if patient test positive what is probability that they have cancer": [
    "example medical diagnosi"
  ],
  "": [
    "example medical diagnosi",
    "example light bulb defect",
    "generative model"
  ],
  "answer there two random variable cancer true or false test positive or negative probability is called prior because it represent my estimate probability before we have done test or made some other observation sensitivity specificity interpreted follow": [
    "example medical diagnosi"
  ],
  "p positive cancer 0 98 p negative cancer 0 97": [
    "example medical diagnosi"
  ],
  "question you work lighting company which manufacture 60 it light bulb in factory 40 in factory b one percent light bulb from factory defective while two percent those from factory b defective if random light bulb turn out be defective what is probability that it wa manufactured in factory": [
    "example light bulb defect"
  ],
  "answer there two random variable factory or b defect ye or no in thi case prior is": [
    "example light bulb defect"
  ],
  "p 0 6 p b 0 4": [
    "example light bulb defect"
  ],
  "conditional probability": [
    "example light bulb defect"
  ],
  "p defect 0 01 p defect b 0 02": [
    "example light bulb defect"
  ],
  "p h d probability that h is correct given that datum d were observed": [
    "baye rule in machine learning"
  ],
  "baye theorem": [
    "baye rule in machine learning"
  ],
  "p h d p d p d h p h": [
    "baye rule in machine learning"
  ],
  "p h d p d h p h p d": [
    "baye rule in machine learning"
  ],
  "p h is called prior": [
    "baye rule in machine learning"
  ],
  "compute matrix second derivative 2e wi wj called hessian": [
    "conjugate gradient"
  ],
  "approximate landscape quadratic function paraboloid": [
    "conjugate gradient"
  ],
  "jump minimum thi quadratic function": [
    "conjugate gradient"
  ],
  "use method from information geometry find natural re scaling partial derivative": [
    "natural gradient"
  ],
  "geometry hidden unit activation": [
    "geometry hidden unit outline"
  ],
  "limitation 2 layer network": [
    "geometry hidden unit outline"
  ],
  "alternative transfer function 6 3": [
    "geometry hidden unit outline"
  ],
  "dropout": [
    "geometry hidden unit outline"
  ],
  "hu space": [
    "n 2 n encoder"
  ],
  "exercise draw hidden unit space 2 2 2 3 2 3 4 2 4 5 2 5 encoder": [
    "8 3 8 encoder"
  ],
  "represent input hidden weight each input unit point hidden output weight each output unit line": [
    "8 3 8 encoder"
  ],
  "now consider 8 3 8 encoder it 3 dimensional hidden unit space what shape would be formed 8 point representing input hidden weight 8 input unit what shape would be formed plane representing hidden output weight each output unit": [
    "8 3 8 encoder"
  ],
  "hint think two platonic solid which dual each other": [
    "8 3 8 encoder"
  ],
  "used visualize higher dimension": [
    "hinton diagram"
  ],
  "white positive black negative": [
    "hinton diagram"
  ],
  "swap any pair hidden node overall function will be same": [
    "symmetry"
  ],
  "any hidden node reverse sign all incoming outgoing weight assuming symmetric transfer function": [
    "symmetry"
  ],
  "hidden node identical input hidden weight in theory would never separate so they all have begin different small random weight": [
    "symmetry"
  ],
  "in practice all hidden node try do similar job at first then gradually specialize": [
    "symmetry"
  ],
  "small weight each layer implement an approximately linear function so multiple layer also implement an approximately linearfunction": [
    "controlled nonlinearity"
  ],
  "large weight transfer function approximate step function so computation become digital learning become very slow": [
    "controlled nonlinearity"
  ],
  "typical weight value two layer neural network implement function which is close linear but take advantage limited degree nonlinearity": [
    "controlled nonlinearity"
  ],
  "some function cannot be learned 2 layer sigmoidal network": [
    "limitation two layer neural network"
  ],
  "example thi twin spiral problem cannot be learned 2 layer network but it can be learned using 3 layer network if we include shortcut connection between non consecutive layer": [
    "limitation two layer neural network"
  ],
  "twin spiral can be learned 3 layer network shortcutconnection": [
    "adding hidden layer"
  ],
  "first hidden l ayer learn linearly separable feature": [
    "adding hidden layer"
  ],
  "second hidden layer learn convex feature": [
    "adding hidden layer"
  ],
  "output layer combine these produce concave feature": [
    "adding hidden layer"
  ],
  "training 3 layer network is delicate": [
    "adding hidden layer"
  ],
  "learning rate initial weight value must be very small": [
    "adding hidden layer"
  ],
  "otherwise network will converge local optimum": [
    "adding hidden layer"
  ],
  "training backpropagation in network many layer is difficult": [
    "vanishing or exploding gradient"
  ],
  "when weight small differential become smaller smallera we backpropagate through layer end up having no effect": [
    "vanishing or exploding gradient"
  ],
  "when weight large activation in higher layer will saturate extreme value result gradient at those layer will become very small will not be propagated earlier layer": [
    "vanishing or exploding gradient"
  ],
  "when weight have intermediate value differential will sometime get multiplied many time is place where transfer function is steep causing them blow up large value": [
    "vanishing or exploding gradient"
  ],
  "layerwise unsupervised pre training": [
    "solution vanishing gradient"
  ],
  "long short t erm memory lstm": [
    "solution vanishing gradient"
  ],
  "new activation function": [
    "solution vanishing gradient"
  ],
  "lstm is specifically recurrent neural network": [
    "solution vanishing gradient"
  ],
  "we will discus unsupervised pre training lstm later in course": [
    "solution vanishing gradient"
  ],
  "sigmoid hyperbolic tangent traditionally used 2 layer network but suffer from vanishing gradient problem in deeper network": [
    "activation function"
  ],
  "rectified linear unit relu popular deep network including convolutional network gradient don t vanish but their highly linear nature may cause other problem": [
    "activation function"
  ],
  "scaled exponential linear unit selu recent innovation which seem work well very deep network": [
    "activation function"
  ],
  "node randomly chosen not be used some fixed probability usually one half": [
    "dropout"
  ],
  "when training is finished network is deployed all node used but their activation multiplied same probability that wa used in dropout": [
    "dropout"
  ],
  "thu activation received each unit is average value what it would have received during training": [
    "dropout"
  ],
  "dropout force network achieve redundancy because it must deal situation where some feature missing": [
    "dropout"
  ],
  "another way view dropout is that it implicitly efficiently simulate an ensemble different architecture": [
    "dropout"
  ],
  "ensembling is method where number different classifier trained same task final clas is decided voting among them": [
    "ensembling"
  ],
  "in order benefit from ensembling we need have diversity in different classifier": [
    "ensembling"
  ],
  "example we could train three neural network different architecture three support vector machine different dimensionsand kernel well two other classifier ensemble all them produce final result": [
    "ensembling"
  ],
  "kaggle competition entry often done in thi way": [
    "ensembling"
  ],
  "diversity can also be achieved training different subset datum": [
    "bagging"
  ],
  "suppose we given n training item": [
    "bagging"
  ],
  "each time we train new classifier we choose n item from training set replacement thi mean that some item will not be chosen while other chosen two or three time": [
    "bagging"
  ],
  "there will be diversity among resulting classifier because they have each been trained different subset datum they can be ensembled produce more accurate result than single classifier": [
    "bagging"
  ],
  "in case dropout same datum used each time but different architecture is created removing node that dropped": [
    "dropout an implicit ensemble"
  ],
  "trick multiplying output each node probability dropout implicitly average output over all these different model": [
    "dropout an implicit ensemble"
  ],
  "convolution layer extract shift invariant feature from previou layer": [
    "convolutional network component"
  ],
  "subsampling or pooling layer combine activation multiple unit from previou layer into one unit": [
    "convolutional network component"
  ],
  "fully connected layer collect spatially diffuse information": [
    "convolutional network component"
  ],
  "output layer choose between class": [
    "convolutional network component"
  ],
  "there can be multiple step convolution followed pooling before reaching fully connected layer": [
    "convolutional network architecture"
  ],
  "note how pooling reduce size feature map usually half in each direction": [
    "convolutional network architecture"
  ],
  "consider classification task n clas assume zi outputof unit corresponding class j": [
    "softmax"
  ],
  "we assume network estimate t he probability each clas j isproportional exp z because probabilite must add up 1 weneed normalize dividing their sum": [
    "softmax"
  ],
  "prob i exp zi nj 1 exp zj": [
    "softmax"
  ],
  "log prob i zi log nj 1 exp zj": [
    "softmax"
  ],
  "if correct clas is i we can treat logprob i my cost function first term push up correct clas i while second term mainlypush down i ncorrect clas j highest activation if j i": [
    "softmax"
  ],
  "example in first convolutional layer lenet j k 32 m n 5": [
    "example lenet"
  ],
  "width next layer isj 1 m 32 1 5 28": [
    "example lenet"
  ],
  "question if there 6 filter in t hi layer compute number weight per neuron neuron connection independent parameter": [
    "example lenet"
  ],
  "5 5 window first convolution layer extract from original 32 32 image 28 28 array feature subsampling then half thi size 14 14 second c onvolution layer use another 5 5 window extract 10 10 array feature which second subsampling layer reduce 5 5 these activation then pas through two fully connectedlayer into 10 output unit corresponding digit 0 9": [
    "example lenet trained mnist"
  ],
  "sometime we treat off edge input zero or some other value": [
    "convolution zero padding"
  ],
  "thi is known zero padding": [
    "convolution zero padding"
  ],
  "zero padding convolution layer is same size originalimage or previou layer": [
    "convolution zero padding"
  ],
  "5 convolutional layer 3 fully connected layer": [
    "example alexnet",
    "alexnet architecture"
  ],
  "max pooling overlapping stride": [
    "example alexnet",
    "alexnet architecture"
  ],
  "softmax 1000 class": [
    "example alexnet",
    "alexnet architecture"
  ],
  "2 parallel gpu which interact only at certain layer": [
    "example alexnet",
    "alexnet architecture"
  ],
  "assume original image is j k l channel we again apply an m n filter but thi ti me stride 1 in thi example j 7 k 9 l 3 m 3 n 3 s 2": [
    "stride"
  ],
  "zj k g b m 0 n 0 kl m nvj m k n same formula is used but j k now incremented each time number free parameter is 1 l m n": [
    "stride"
  ],
  "j take value 0 2 j m": [
    "stride dimension"
  ],
  "k take value 0 2 k n": [
    "stride dimension"
  ],
  "next layer is 1 j m 1 k n s": [
    "stride dimension"
  ],
  "when combined zero padding width p": [
    "stride zero padding"
  ],
  "j take value 0 2 j 2p m": [
    "stride zero padding"
  ],
  "k take value 0 2 k 2p n": [
    "stride zero padding"
  ],
  "next layer is 1 j 2p m 1 k 2p n s": [
    "stride zero padding"
  ],
  "example in first convolutional layer alexnet": [
    "example alexnet conv layer 1"
  ],
  "j k 224 p 2 m n 11 4": [
    "example alexnet conv layer 1"
  ],
  "width next layer is": [
    "example alexnet conv layer 1"
  ],
  "1 j 2p m 1 224 2 2 11 4 55": [
    "example alexnet conv layer 1"
  ],
  "question if there 96 filter in thi layer compute number weight per neuron neuron connection independent parameter": [
    "example alexnet conv layer 1"
  ],
  "if previou layer is j k max pooling is applied width f stride size next layer will be": [
    "overlapping pooling"
  ],
  "1 j f 1 k f s": [
    "overlapping pooling"
  ],
  "question if max pooling width 3 stride 2 is applied feature size 55 55 in first convolutional layer alexnet what is size next layer": [
    "overlapping pooling"
  ],
  "answer question how many independent parameter do t hi add model": [
    "overlapping pooling"
  ],
  "image dataset task": [
    "image processing outline"
  ],
  "convolution in detail": [
    "image processing outline"
  ],
  "alexnet": [
    "image processing outline"
  ],
  "weight initialization": [
    "image processing outline",
    "10 layer",
    "dealing deep network",
    "dealing deep network"
  ],
  "batch normalization": [
    "image processing outline"
  ],
  "residual network": [
    "image processing outline"
  ],
  "dense network": [
    "image processing outline"
  ],
  "style transfer": [
    "image processing outline",
    "image processing task"
  ],
  "black white resolution 28 28": [
    "mnist handwritten digit dataset"
  ],
  "60 000 image": [
    "mnist handwritten digit dataset"
  ],
  "10 class 0 1 2 3 4 5 6 7 8 9": [
    "mnist handwritten digit dataset"
  ],
  "color resolution 32 32": [
    "cifar image dataset"
  ],
  "50 000 image": [
    "cifar image dataset"
  ],
  "10 class": [
    "cifar image dataset"
  ],
  "color resolution 227 227": [
    "imagenet lsvrc dataset"
  ],
  "1 2 million image": [
    "imagenet lsvrc dataset"
  ],
  "1000 class": [
    "imagenet lsvrc dataset"
  ],
  "image classification": [
    "image processing task"
  ],
  "object detection": [
    "image processing task"
  ],
  "object segmentation": [
    "image processing task"
  ],
  "generating image": [
    "image processing task"
  ],
  "generating art": [
    "image processing task"
  ],
  "image captioning": [
    "image processing task"
  ],
  "5 5 window first convolution layer extract from original32 32 image 28 28 array feature subsampling then half thissize 14 14 second convolution layer use another 5 5 windowto extract 10 10 array feature which second subsampling layerreduce 5 5 these activation then pas through two fully connectedlayer into 10 output unit corresponding digit 0 9": [
    "lenet trained mnist"
  ],
  "alexnet 8 layer 2012": [
    "imagenet architecture"
  ],
  "vgg 19 layer 2014": [
    "imagenet architecture"
  ],
  "googlenet 22 layer 2014": [
    "imagenet architecture"
  ],
  "resnet 152 layer 2015": [
    "imagenet architecture"
  ],
  "650k neuron": [
    "alexnet detail"
  ],
  "630m connection": [
    "alexnet detail"
  ],
  "60m parameter": [
    "alexnet detail"
  ],
  "more parameter that image danger overfitting": [
    "alexnet detail"
  ],
  "rectified linear unit relu": [
    "enhancement"
  ],
  "overlapping pooling width 3 stride 2": [
    "enhancement"
  ],
  "stochastic gradient descent momentum weight decay": [
    "enhancement"
  ],
  "datum augmentation reduce overfitting": [
    "enhancement"
  ],
  "50 dropout in fully connected layer": [
    "enhancement"
  ],
  "ten patch size 224 224 cropped from each original 227 277 image using zero padding": [
    "datum augmentation"
  ],
  "horizontal re ection each patch is also included": [
    "datum augmentation"
  ],
  "at test time average prediction 10 patch": [
    "datum augmentation"
  ],
  "also include change in intensity rgb channel": [
    "datum augmentation"
  ],
  "filter gpu 1 upper color agnostic": [
    "convolution kernel"
  ],
  "filter gpu 2 lower color specific": [
    "convolution kernel"
  ],
  "these resemble gabor filter": [
    "convolution kernel"
  ],
  "batch nomalization": [
    "10 layer",
    "dealing deep network",
    "dealing deep network"
  ],
  "skip connection": [
    "30 layer",
    "dealing deep network",
    "dealing deep network"
  ],
  "identity skip connection": [
    "100 layer",
    "dealing deep network",
    "dealing deep network"
  ],
  "10 layer": [
    "dealing deep network"
  ],
  "30 layer": [
    "dealing deep network"
  ],
  "100 layer": [
    "dealing deep network"
  ],
  "example tos coin once count number head": [
    "statistic example coin tossing"
  ],
  "mean 1 2 0 1 0 5": [
    "statistic example coin tossing"
  ],
  "variance 1 2 0 0 5 2 1 0 5 2 0 25": [
    "statistic example coin tossing"
  ],
  "standard deviation variance 0 5": [
    "statistic example coin tossing"
  ],
  "example tos coin 100 time count number head": [
    "statistic example coin tossing"
  ],
  "mean 100 0 5 50variance 100 0 25 25": [
    "statistic example coin tossing"
  ],
  "standard deviation variance 5": [
    "statistic example coin tossing"
  ],
  "example tos coin 10000 time count number head": [
    "statistic example coin tossing"
  ],
  "5000 2500 50": [
    "statistic example coin tossing"
  ],
  "if we simply stack additional layer it can lead higher training errora well higher test error": [
    "going deeper"
  ],
  "idea take any two consecutive stacked layer in deep network add skip connection which bipass these layer is added their output": [
    "residual network"
  ],
  "preceding layer attempt do whole job making x closea possible target output entire network": [
    "residual network"
  ],
  "f x is residual component which correct error from previouslayer or provide additional detail which previou layer werenot powerful enough compute": [
    "residual network"
  ],
  "skip connection both training test error drop you add more layer": [
    "residual network"
  ],
  "more than 100 layer need apply relu before addingthe residual instead afterward thi is called anidentity skipconnection": [
    "residual network"
  ],
  "recently good result have been achieved using network denselyconnected block within which each layer is connected shortcutconnection all preceding layer": [
    "dense network"
  ],
  "content style new image": [
    "neural style transfer"
  ],
  "imagenet classificationwithdeepconvolutionalneuralnetwork krizhevsky et al 2015": [
    "reference"
  ],
  "understanding difficulty training deep feedforward neuralnetwork glorot bengio 2010": [
    "reference"
  ],
  "batch normalization accelerating deep network training byreducing internal covariate shift ioffe szegedy icml 2015": [
    "reference"
  ],
  "deep residual learning image recognition he et al 2016": [
    "reference"
  ],
  "densely connected convolutional network huang et al 2016": [
    "reference"
  ],
  "neural algorithm artistic style gaty et al 2015 t mikolov k chen g corrado j dean 2013": [
    "reference"
  ],
  "efficient estimationof word representation in vector space": [
    "reference"
  ],
  "arxiv preprint arxiv 1301 3781 t mikolov i sutskever k chen g corrado j dean 2013": [
    "reference"
  ],
  "di tributed representation word phrase their compositionality nip 2013 3111 19 xinrong 2014": [
    "reference"
  ],
  "word2vec parameterlearningexplained arxiv 1411 2738": [
    "reference"
  ],
  "http nlp stanford edu project glove": [
    "reference"
  ],
  "http devblog nvidium com parallelforall introduction neural machine translation gpu part 3": [
    "reference"
  ],
  "david silver deep reinforcement learning tutorial http icml cc 2016 tutorial deeprl tutorial pdf": [
    "reference"
  ],
  "brief survey deep reinforcement learning http arxiv org ab 1708 05866": [
    "reference"
  ],
  "asynchronou method deep r einforcement learning http arxiv org ab 1602 01783": [
    "reference"
  ],
  "evolution strategy scalable alternative r einforcementlearning http arxiv org ab 1703 03864": [
    "reference"
  ],
  "eric jang beginner guide variational method http blog evjang com 2016 08 variational baye html": [
    "reference"
  ],
  "processing temporal sequence": [
    "recurrent network outline",
    "recurrent network"
  ],
  "sliding window": [
    "recurrent network outline",
    "recurrent network"
  ],
  "recurrent network architecture": [
    "recurrent network outline",
    "recurrent network"
  ],
  "there many task which require sequence input be processed rather than single input": [
    "processing temporal sequence"
  ],
  "speech recognition": [
    "processing temporal sequence"
  ],
  "time series prediction": [
    "processing temporal sequence"
  ],
  "machine translation": [
    "processing temporal sequence"
  ],
  "how can neural network model be adapted these task": [
    "processing temporal sequence"
  ],
  "simplest way feed temporal input neural network is sliding window approach first used in nettalk system sejnowski rosenberg 1987": [
    "sliding window"
  ],
  "given sequence 7 character predict phonetic pronunciation ofthe middle character": [
    "nettalk task"
  ],
  "thi task we need know character both side": [
    "nettalk task"
  ],
  "example how vowel in these word pronounced": [
    "nettalk task"
  ],
  "pa pat pate paternal": [
    "nettalk task"
  ],
  "mo mod mode modern": [
    "nettalk task"
  ],
  "nettalk gained lot medium attention at time": [
    "nettalk"
  ],
  "hooking it up speech synthesizer wa very cute in earlystage training it sounded like babbling baby when fully trained it pronounced word mostly correctly but sounded somewhat robotic": [
    "nettalk"
  ],
  "later study similar task have often found that decision treecould produce equally good or better accuracy": [
    "nettalk"
  ],
  "thi kind approach can only learn short term dependency not themedium or long term dependency that required some task": [
    "nettalk"
  ],
  "we can unroll recurrent architecture into an equivalent feed forward architecture shared weight": [
    "back propagation through time"
  ],
  "applying backpropagation unrolled architecture is reffered backpropagation through time": [
    "back propagation through time"
  ],
  "we can backpropagate just one timestep or fixed number timestep or all way back beginning sequence": [
    "back propagation through time"
  ],
  "it is sometime beneficial add shortcut connection directly from input output": [
    "other recurrent network architecture"
  ],
  "connection from output back hidden have also been explored sometime called jordan network": [
    "other recurrent network architecture"
  ],
  "gated network trained bptt": [
    "dynamical recognizer"
  ],
  "emulate exactly behavimy finite state automaton": [
    "dynamical recognizer"
  ],
  "trained network emulate behavimy finite state automaton": [
    "dynamical recognizer"
  ],
  "training set must include short medium long example": [
    "dynamical recognizer"
  ],
  "language machine example": [
    "chomsky hierarchy"
  ],
  "regular finite state automaton n odd": [
    "chomsky hierarchy"
  ],
  "context free push down automaton ab": [
    "chomsky hierarchy"
  ],
  "context sensitive linear bounded automaton abc": [
    "chomsky hierarchy"
  ],
  "recursively enumerable turing machine true qbf": [
    "chomsky hierarchy"
  ],
  "abaabbabaaabbbaaaabbbbabaabbaaaaabbbbb": [
    "task formal language prediction"
  ],
  "scan sequence character one at time try at each step predict next character in sequence": [
    "task formal language prediction"
  ],
  "in some case prediction is probabilistic": [
    "task formal language prediction"
  ],
  "abtask first b is not predictable but subsequent b initial in next subsequence predictable": [
    "task formal language prediction"
  ],
  "network do not implement finite state automaton but insteaduse two fixed point in activation space one attracting other repelling wile elman 1995": [
    "learning predict anbn"
  ],
  "network trained only up 10b 10 could generalize up 12b 12": [
    "learning predict anbn"
  ],
  "training weight evolution is more stable than backpropagation": [
    "learning predict anbn"
  ],
  "network trained evolution were sometime monotonic rather than oscillating": [
    "learning predict anbn"
  ],
  "hidden unit trajectory": [
    "hidden unit analysi anbn"
  ],
  "fixed point eigenvector": [
    "hidden unit analysi anbn"
  ],
  "thi task sequence is accepted if number b s equal": [
    "counting spiralling"
  ],
  "network count up spiralling inward down spiralling outward": [
    "counting spiralling"
  ],
  "srn 3 hidden unit can learn predict abcby counting up anddown simultaneously in different direction thu producing star shape": [
    "hidden unit dynamic anbncn"
  ],
  "simple recurrent network srn can learn medium rangedependency but have difficulty learning long range dependency": [
    "long range dependency"
  ],
  "long short term memory lstm gated recurrent unit gru can learn long range dependency better than srn": [
    "long range dependency"
  ],
  "at each time step hidden layer activation copied context layer": [
    "simple recurrent network"
  ],
  "hidden layer receife connection from input context layer": [
    "simple recurrent network"
  ],
  "input fed one at time t o network it use context layerto remember whatever information is required it produce thecorrect output": [
    "simple recurrent network"
  ],
  "srn context layer is combined directly i nput produce thenext hidden layer": [
    "simple recurrent network"
  ],
  "srn can learn reber grammar but not embedded reber grammar": [
    "simple recurrent network"
  ],
  "two excellent web resource lstm": [
    "long short term memory",
    "lstm"
  ],
  "http colah github io post 2015 08 understanding lstm": [
    "long short term memory",
    "lstm"
  ],
  "christianhertum de lehre datascience machinelearning neuralnetwork lstm php": [
    "long short term memory",
    "lstm"
  ],
  "lstm context layer is modulated three gating mechanism forget gate input gate output gate http colah github io post 2015 08 understanding lstm": [
    "long short term memory",
    "lstm"
  ],
  "gru is similar lstm but ha only two gate instead three": [
    "gated recurrent unit",
    "gru"
  ],
  "statistical language processing": [
    "language processing outline"
  ],
  "synonym elegant": [
    "language processing outline",
    "statistical language processing"
  ],
  "stylish graceful tasteful discerning refined sophisticated dignified cultivated distinguished classic smart fashionable modish decorou beautiful artistic aesthetic lovely charming polished suave urbane cultured dashing debonair luxuriou sumptuou opulent grand plush high clas exquisite": [
    "language processing outline",
    "statistical language processing"
  ],
  "synonym antonym taxonomy require human effort may beincomplete require discrete choice nuance lost word like king queen can be similar in some attribute but opposite in other could we instead extract some statistical property automatically withouthuman involvement": [
    "language processing outline",
    "statistical language processing"
  ],
  "n gram model": [
    "language processing outline"
  ],
  "co occurence matrix": [
    "language processing outline"
  ],
  "word representation": [
    "language processing outline"
  ],
  "word2vec": [
    "language processing outline",
    "word2vec glove"
  ],
  "predictive model": [
    "language processing outline",
    "word2vec",
    "word2vec glove",
    "word2vec glove"
  ],
  "maximize probability word based surrounding word": [
    "language processing outline",
    "word2vec",
    "word2vec glove",
    "word2vec glove"
  ],
  "word relationship": [
    "language processing outline"
  ],
  "neural machine translation": [
    "language processing outline"
  ],
  "combining image language": [
    "language processing outline"
  ],
  "penguin is bird is mammal is vertebrate": [
    "taxonomy",
    "word meaning synonym taxonomy",
    "word meaning synonym taxonomy"
  ],
  "what is meaning meaning": [
    "word meaning synonym taxonomy"
  ],
  "dictionary definition": [
    "word meaning synonym taxonomy"
  ],
  "synonym antonym": [
    "word meaning synonym taxonomy"
  ],
  "taxonomy": [
    "word meaning synonym taxonomy"
  ],
  "there wa crooked man who walked crooked mile": [
    "there wa crooked man"
  ],
  "found crooked sixpenceupon crooked stile": [
    "there wa crooked man"
  ],
  "he bought crooked cat who caught crooked mouse": [
    "there wa crooked man"
  ],
  "they all lived togetherin little crooked house": [
    "there wa crooked man"
  ],
  "some word occur frequently in all or most document": [
    "counting frequency"
  ],
  "some word occur frequently in particulardocument but not generally": [
    "counting frequency"
  ],
  "thi information can be useful documentclassification": [
    "counting frequency"
  ],
  "each column matrix become vector representing thecorresponding document": [
    "document classification"
  ],
  "word like cat mouse house tend occur in child booksor rhyme": [
    "document classification"
  ],
  "other group word may be characteristic legal document political news sporting result etc": [
    "document classification"
  ],
  "word occurring many time in one document may skew vector might be better just have 1 or 0 indicating whether wordoccur at all": [
    "document classification"
  ],
  "normalizing each row sum 1 we can estimate probabilityprob w w word woccurring after w": [
    "n gram model"
  ],
  "need aggregrate over large corpu so that unusual word like crooked will not dominate": [
    "n gram model"
  ],
  "model capture some common combination like there wa man who found he bought who caught they they all lived together etc": [
    "n gram model"
  ],
  "thi unigram model can be generalized bi gram trus gram n gram model considering n preceding word": [
    "n gram model"
  ],
  "if vocabulary is large we need some trick avoid exponentialuse memory": [
    "n gram model"
  ],
  "rashly good night is very liberal it is easily said there is gyved asore distraction in wrath my king may choose but none shapesand editing thi show sea what thi is miching malhecho gin me pas transport hi wit hamlet my arm againstthe mind impatient condition that would fain know which thewicked deed get from deed ymy tutor": [
    "1 gram text generator"
  ],
  "is there way reduce dimensionality while still preserving": [
    "each row thi matrix could be considered vector representation"
  ],
  "sometime we don t necessarily predict next word but simply nearby word e g word occurring within an n word windowcentered that word": [
    "co occurrence matrix"
  ],
  "we can build matrix in which each row represent word eachcolumn nearby word": [
    "co occurrence matrix"
  ],
  "each row thi matrix could be considered vector representation corresponding word but number dimension is equal tothe size vocabulary which could be very large 10 5": [
    "co occurrence matrix"
  ],
  "is there way reduce dimensionality while still preserving relationship between word": [
    "co occurrence matrix"
  ],
  "aggregating over many document pair or group wordsemerge which tend occur near each other but not necessarilyconsecutively": [
    "co occurrence matrix"
  ],
  "cat caught mouse": [
    "co occurrence matrix",
    "aggregating over many document pair or group word"
  ],
  "walked mile": [
    "co occurrence matrix",
    "aggregating over many document pair or group word"
  ],
  "little house": [
    "co occurrence matrix",
    "aggregating over many document pair or group word"
  ],
  "common word tend dominate matrixrelationship les common word could we sample common word les often in order reveal relationship between word": [
    "co occurrence matrix"
  ],
  "could we sample common word les often in order reveal": [
    "common word tend dominate matrix"
  ],
  "word that used occur in same context tend purport similar meaning z harri 1954": [
    "word embedding"
  ],
  "you shall know word company it keep j r firth 1957": [
    "word embedding"
  ],
  "aim word embedding": [
    "word embedding"
  ],
  "find vector representation each word such that word withnearby representation likely occur in similar context": [
    "word embedding"
  ],
  "structuralist linguistic firth 1957": [
    "history word embedding"
  ],
  "recurrent network rumelhart hinton william 1986": [
    "history word embedding"
  ],
  "latent semantic analysi deerwester et al 1990": [
    "history word embedding"
  ],
  "hyperspace analogue language lund burges atchley 1995": [
    "history word embedding"
  ],
  "neural probabilistic language model bengio 2000": [
    "history word embedding"
  ],
  "nlp almost from scratch collobert et al 2008": [
    "history word embedding"
  ],
  "word2vec mikolov et al 2013": [
    "history word embedding"
  ],
  "glove pennington socher manning 2014": [
    "history word embedding"
  ],
  "count based model": [
    "glove",
    "word2vec glove"
  ],
  "reconstruct close approximation co occurrence matrix x": [
    "glove",
    "word2vec glove"
  ],
  "typically l is number word in vocabulary 60 000 m is either equal l or in case document classification number document in collection svd is computationallyexpensive proportional l mif l m can we generate word vectorsin similar way but les computation incrementally": [
    "word2vec glove"
  ],
  "glove": [
    "word2vec glove"
  ],
  "if x is symmetric positive semi definite eigenvalue singularvalue decomposition same": [
    "eigenvalue vs singular value decomposition"
  ],
  "in general eigenvalue can be negative or even complex but singularvalue alway real non negative": [
    "eigenvalue vs singular value decomposition"
  ],
  "even if x is square matrix singular value decompositon treat thesource target two entirely different space": [
    "eigenvalue vs singular value decomposition"
  ],
  "word co occurrence matrix is symmetric but not positive semi definite example if text consisted entirely two alternatingletter ababababababab then would be context b vice versa": [
    "eigenvalue vs singular value decomposition"
  ],
  "krow vof w is representation word k": [
    "word2vec 1 word context model"
  ],
  "jcolumn vof wi an alternative representation word j": [
    "word2vec 1 word context model"
  ],
  "if 1 hot input is k linear sum at each output will be u vv": [
    "word2vec 1 word context model"
  ],
  "continuou bag word": [
    "thi 1 word prediction model can be extended multus word prediction in two different way",
    "word2vec issue",
    "word2vec issue"
  ],
  "skip gram": [
    "thi 1 word prediction model can be extended multus word prediction in two different way",
    "word2vec issue",
    "word2vec issue"
  ],
  "hierarchical softmax": [
    "why need computationally efficient alternative softmax",
    "word2vec issue",
    "word2vec issue"
  ],
  "negative sampling": [
    "why need computationally efficient alternative softmax",
    "word2vec issue",
    "word2vec issue"
  ],
  "word2vec is linear model in sense that there is no activation function at hidden node": [
    "word2vec issue"
  ],
  "thi 1 word prediction model can be extended multus word prediction in two different way": [
    "word2vec issue"
  ],
  "why need computationally efficient alternative softmax": [
    "word2vec issue"
  ],
  "need sample frequent word les often": [
    "word2vec issue"
  ],
  "if several context word eachused independently predict thecenter word hidden activationbecome sum or average over allthe context word": [
    "continuou bag word"
  ],
  "note difference betweenthi nettalk in word2vec cbow all context word sharethe same input hidden weight": [
    "continuou bag word"
  ],
  "try predict context word given center word": [
    "word2vec skip gram model"
  ],
  "thi skip gram model is similar tocbow except that in thi case asingle input word is used predictmultiple context word": [
    "word2vec skip gram model"
  ],
  "all context word share samehidden output weight": [
    "word2vec skip gram model"
  ],
  "target word organized in huffman coded binary tree": [
    "hierarchical softmax"
  ],
  "each output network correspond one branch point in tree": [
    "hierarchical softmax"
  ],
  "only those node that visited along path target word areevaluated which is log v node average": [
    "hierarchical softmax"
  ],
  "idea negative sampling is that we train network increaseit estimation target word jand reduce it estimate not all theword in vocabulary but just subset them w drawn from anappropriate distribution": [
    "negative sampling"
  ],
  "e log vh log vh": [
    "negative sampling"
  ],
  "thi is simplified version noise constrastive estimation nce it is not guaranteed produce well defined probability distribution but in practice it do produce high quality word embedding": [
    "negative sampling"
  ],
  "number sample is 5 20 small dataset 2 5 largedataset": [
    "negative sampling"
  ],
  "empirically good choice distribution from which draw thenegative sample is p w u w z where u w is unigramdistribution determined previou word z is normalizingconstant": [
    "negative sampling"
  ],
  "in order diminish in uence more frequent word each word inthe corpu is discarded probability": [
    "subsampling frequent word"
  ],
  "p w 1 t f w": [
    "subsampling frequent word"
  ],
  "where f w is frequency word wand t 10i an empiricallydetermined threshold": [
    "subsampling frequent word"
  ],
  "q1 seeing picture my old home made me feel nostalgic": [
    "sentence completion task"
  ],
  "fastidiou b indignant c wistful d conciliatory": [
    "sentence completion task"
  ],
  "q2 because house had vote override presidential veto president ha no choice but": [
    "sentence completion task"
  ],
  "object b abdicate c abstain d compromise": [
    "sentence completion task"
  ],
  "use model choose which word is most likely occur in thi context": [
    "sentence completion task"
  ],
  "king woman man queen": [
    "linguistic regularity"
  ],
  "more generally is b c is": [
    "linguistic regularity"
  ],
  "d argmax v v v tv vc vb va": [
    "linguistic regularity"
  ],
  "q1 evening is morning dinner is": [
    "word analogy task"
  ],
  "breakfast b soup c coffee d time": [
    "word analogy task"
  ],
  "q2 bow is arrow is bullet": [
    "word analogy task"
  ],
  "defend b lead c shoot d gun": [
    "word analogy task"
  ],
  "reinforcement learning vs supervised learning": [
    "reinforcement learning outline"
  ],
  "model optimality": [
    "reinforcement learning outline"
  ],
  "exploration vs exploitation": [
    "reinforcement learning outline"
  ],
  "value function learning": [
    "reinforcement learning outline",
    "rl approach",
    "reinforcement learning approach"
  ],
  "policy learning": [
    "reinforcement learning outline",
    "rl approach",
    "reinforcement learning approach"
  ],
  "actor critic": [
    "reinforcement learning outline",
    "rl approach",
    "reinforcement learning approach",
    "deep reinforcement learning outline"
  ],
  "td learning": [
    "value function learning",
    "rl approach",
    "reinforcement learning approach"
  ],
  "q learning": [
    "value function learning",
    "rl approach",
    "reinforcement learning approach"
  ],
  "every policy determine value function v r where v s isthe average discounted reward received an agent who begin in state sand choose it action according policy if is optimal then v s v s is maximum expected discounted reward obtainable from state s learning t hi optimal valuefunction can help determine optimal strategy agent retain it ownestimate v true value function v aim value function learning is generally start random vand t hen iteratively improve it so that it more closely approximate v thi proces is sometime called bootstrapping": [
    "value function learning"
  ],
  "hill climbing": [
    "policy learning",
    "rl approach",
    "reinforcement learning approach"
  ],
  "policy gradient": [
    "policy learning",
    "rl approach",
    "reinforcement learning approach"
  ],
  "evolutionary strategy": [
    "policy learning",
    "rl approach",
    "reinforcement learning approach",
    "method updating sigma"
  ],
  "there is another clas reinforcement learning algorithm which do not optimize value function but instead try optimize policy itself directly normally we consider family policy determined parameter example weight neural network episodic domain like backgammon we do not need discount factor fitnes policy can be taken value function initial state s0 under thi policy which is expected or average total reward received in each game an agent using policy fitnes v s0 e rtotal": [
    "policy learning"
  ],
  "mobile robot pole balancing yi ng helicopter": [
    "optimal control",
    "learning action"
  ],
  "job shop scheduling mobile phone channel allocation": [
    "resource allocation",
    "learning action",
    "learning action"
  ],
  "elevator control backgammon": [
    "mix allocation control",
    "learning action",
    "learning action"
  ],
  "supervised learning can also be used learn action if we construct atraining set situation action pair called behavioral cloning however there many application which it is difficult inapproprus ate or even impossible provide training set": [
    "learning action"
  ],
  "optimal control": [
    "learning action"
  ],
  "mobile robot pole balancing flying helicopter": [
    "learning action"
  ],
  "resource allocation": [
    "learning action"
  ],
  "mix allocation control": [
    "learning action"
  ],
  "there some environment in which any deterministic agent willperform very poorly optus mal reactive policy must be stochastic i e randomized in 2 player game like r ock paper scissors random strategy is alsorequired in order m ake agent choice unpredictable opponent": [
    "probabilistic policy"
  ],
  "recall fitnes e q log s non episodic game we cannot easily find good estimate forq s one approach is consider family q function qdetermined parameter w different from learn w so thatqapproximate q at t he same t i me that policy itself is alsobeing learned thi is known anactor critic approach because t he policy determinesthe action while q function estimate how good current policy is thereby play role critic": [
    "actor critic"
  ],
  "combination value policy learning": [
    "rl approach",
    "reinforcement learning approach",
    "actor critic"
  ],
  "special case an active stochastic environment only one statei called k armed bandit problem because it is like being in room several friendly slot machine limited time trying tocollect much money possible eachaction slot machine provide different average reward": [
    "k armed bandit problem"
  ],
  "use subsequent position refine evaluation current position": [
    "temporal difference learning",
    "how choose target value"
  ],
  "general method do not rely knowing world model": [
    "temporal difference learning"
  ],
  "theorem q learning will eventually converge optimal policy any deterministic markov decision proces assuming an appropriately randomized strategy watkin dayan 1992 theorem td learning will also converge probability 1 sutton 1988 dayan 1992 dayan sejnowski 1994": [
    "theoretical result"
  ],
  "convergence is slow if search space is large": [
    "search space must be finite",
    "limitation theoretical result"
  ],
  "rely visiting every state infinitely often": [
    "search space must be finite",
    "limitation theoretical result"
  ],
  "need have some kind generalisation e g td gammon": [
    "real world problem we can t rely lookup table",
    "limitation theoretical result"
  ],
  "delayed reinforcementtime step later which also slow down learning": [
    "limitation theoretical result"
  ],
  "reward resulting from an action may not be received until several time step later which also slow down learning": [
    "limitation theoretical result"
  ],
  "search space must be finite": [
    "limitation theoretical result"
  ],
  "real world problem we can t rely lookup table": [
    "limitation theoretical result"
  ],
  "suppose we want write computer program play game like backgammon ches checker or go thi can be done using tree search algorithm expectimax mct or minimax alpha betum pruning but we need an appropriate way encoding any board position set number b way train neural network or other learning system compute aboard evaluation based those number": [
    "computer game playing"
  ],
  "board encoding": [
    "backgammon neural network"
  ],
  "4 unit 2 player 24 point": [
    "backgammon neural network"
  ],
  "2 unit bar": [
    "backgammon neural network"
  ],
  "2 unit off board": [
    "backgammon neural network"
  ],
  "two layer neural network": [
    "backgammon neural network"
  ],
  "196 input unit": [
    "backgammon neural network"
  ],
  "20 hidden unit": [
    "backgammon neural network"
  ],
  "1 output unit": [
    "backgammon neural network"
  ],
  "input is encoded board position state output v is value thi position probability winning at each move roll dice find all possible next board position convert them appropriate input format feed them network choose one which produce largest output": [
    "backgammon neural network"
  ],
  "learn move from human game expert preference": [
    "behavioral cloning supervised learning",
    "how choose target value"
  ],
  "td root td leaf mct treestrap": [
    "method which combine learning tree search",
    "how choose target value"
  ],
  "behavioral cloning supervised learning": [
    "how choose target value"
  ],
  "temporal difference learning rule game": [
    "how choose target value"
  ],
  "general method do not rely knowing world model rule game": [
    "how choose target value"
  ],
  "method which combine l earning tree search must know world model": [
    "how choose target value"
  ],
  "ep network wa trained expert preference supervised": [
    "tesauro trained two network",
    "td gammon"
  ],
  "td network wa trained self play td learning": [
    "tesauro trained two network",
    "td gammon"
  ],
  "tesauro trained two network": [
    "td gammon"
  ],
  "td network outperformed ep network": [
    "td gammon"
  ],
  "modification such 3 step lookahead expectimax additional hand crafted input feature td gammon became best backgammon player in world tesauro 1995": [
    "td gammon"
  ],
  "initialize champ policy champ 0": [
    "hill climbing"
  ],
  "each trial generate mutant policy mutant champ gaussian noise fixed": [
    "hill climbing"
  ],
  "champ mutant evaluated same task": [
    "hill climbing"
  ],
  "if mutant do better than champ champ 1 champ mutant": [
    "hill climbing",
    "evolution strategy"
  ],
  "in some case size update is scaled according difference in fitnes may be negative": [
    "hill climbing"
  ],
  "rectangular rink rounded corner": [
    "shock physic"
  ],
  "near frictionles playing surface": [
    "shock physic"
  ],
  "spring method collision handling": [
    "shock physic"
  ],
  "frictionles puck never acquire any spin": [
    "shock physic"
  ],
  "6 braitenberg style sensor equally spaced around vehicle": [
    "shock sensor"
  ],
  "each sensor ha an angular range 90with an overlap 30between neighbouring sensor": [
    "shock sensor"
  ],
  "ball puck": [
    "each 6 sensor respond three different stimuli",
    "shock input",
    "shock input"
  ],
  "own goal": [
    "each 6 sensor respond three different stimuli",
    "shock input",
    "shock input"
  ],
  "opponent goal": [
    "each 6 sensor respond three different stimuli",
    "shock input",
    "shock input"
  ],
  "each 6 sensor respond three different stimuli": [
    "shock input"
  ],
  "3 additional input specify current velocity vehicle": [
    "shock input"
  ],
  "total 3 6 3 21 input": [
    "shock input"
  ],
  "single layer network 21 input 4 output": [
    "shock agent"
  ],
  "total 4 21 1 88 weight": [
    "shock agent"
  ],
  "my genome evolutionary computation consist vector ofthese 88 parameter": [
    "shock agent"
  ],
  "mutation add gaussian random noise each parameter standard deviation 0 05": [
    "shock agent"
  ],
  "random position puck": [
    "each game begin random game initial condition",
    "shock task",
    "shock task"
  ],
  "random position orientation player": [
    "each game begin random game initial condition",
    "shock task",
    "shock task"
  ],
  "1 if puck enemy goal": [
    "each game end",
    "shock task",
    "shock task"
  ],
  "1 if puck own goal": [
    "each game end",
    "shock task",
    "shock task"
  ],
  "0 if time limit expire": [
    "each game end",
    "shock task",
    "shock task"
  ],
  "each game begin random game initial condition": [
    "shock task"
  ],
  "each game end": [
    "shock task"
  ],
  "mutant champ gaussian noise": [
    "evolution strategy"
  ],
  "champ mutant play up n game same game initial condition": [
    "evolution strategy"
  ],
  "better mean mutant must score higher than champ in first game at least high champ in each subsequent game": [
    "evolution strategy"
  ],
  "gradient information provide more precise update particularly": [
    "performance wa almost good td gammon but not quite"
  ],
  "hc gammon wa trained play b ackgammon using thi evolution strategy": [
    "hc gammon"
  ],
  "same game initial condition same seed generating dice roll": [
    "hc gammon"
  ],
  "weight were used determine value function but learning optimize performance policy directly rather than aiming make value function more accurate": [
    "hc gammon"
  ],
  "performance wa almost good td gammon but not quitefor rarely used weight in network": [
    "hc gammon"
  ],
  "gradient information provide more precise update particularly rarely used weight in network": [
    "hc gammon"
  ],
  "history reinforcement learning": [
    "deep reinforcement learning outline"
  ],
  "deep q learning atarus game": [
    "deep reinforcement learning outline"
  ],
  "asynchronou advantage actor critic a3c": [
    "deep reinforcement learning outline"
  ],
  "evolutionary variational method": [
    "deep reinforcement learning outline"
  ],
  "1961 menace tic tac toe donald michie": [
    "model free method",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "1986 td rich sutton": [
    "model free method",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "1989 td gammon gerald tesauro": [
    "model free method",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "2015 deep q learning atarus game": [
    "model free method",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "2016 a3c mnih et al": [
    "model free method",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "2017 openai evolution strategy saliman et al": [
    "model free method",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "1959 checker arthur samuel": [
    "method relying world model",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "1997 td leaf baxter et al": [
    "method relying world model",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "2009 treestrap venes et al": [
    "method relying world model",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "2016 alpha go silver et al": [
    "method relying world model",
    "reinforcement learning timeline",
    "reinforcement learning timeline"
  ],
  "model free method": [
    "reinforcement learning timeline"
  ],
  "method relying world model": [
    "reinforcement learning timeline"
  ],
  "thi box algorithm wa later adapted learn more general task such pole balancing helped lay foundation modern field reinforcement learning": [
    "reinforcement learning box"
  ],
  "variou reason interest in reinforcement learning faded in thelate 70 early 80 s but wa revived in late 1980 s largely through work richard sutton": [
    "reinforcement learning box"
  ],
  "gerald tesauro applied sutton td learning algorithm gameof backgammon in 1989": [
    "reinforcement learning box"
  ],
  "8 bit rgb image 210 160 pixel": [
    "input state is stack raw pixel from last 4 frame",
    "deep q learning atarus game"
  ],
  "end end learning value q from pixel s": [
    "deep q learning atarus game"
  ],
  "input state is stack raw pixel from last 4 frame": [
    "deep q learning atarus game"
  ],
  "8 bitrgbimage 210 160pixel": [
    "deep q learning atarus game"
  ],
  "output is q 18 joystick button position": [
    "deep q learning atarus game"
  ],
  "reward is change in score that timestep": [
    "deep q learning atarus game"
  ],
  "note gradient is applied only q": [
    "instead q"
  ],
  "choose action using current q function greedy": [
    "deep q learning experience replay"
  ],
  "build database experience r s": [
    "deep q learning experience replay",
    "deep q learning experience replay"
  ],
  "sample asynchronously from database apply update minimize r maxq b q s": [
    "deep q learning experience replay"
  ],
  "remove temporal correlation sampling from variety gamesituation in random order": [
    "deep q learning experience replay"
  ],
  "make it easier parallelize algorithm mult ipl e gpu": [
    "deep q learning experience replay"
  ],
  "weight experience according surprise": [
    "prioritised replay",
    "dqn improvement"
  ],
  "instead sampling experience uniformly store them in priority queue according dqn error": [
    "prioritised replay"
  ],
  "rt max qw st 1 b qw st at": [
    "prioritised replay"
  ],
  "thi ensure system will concentrate more effort situation where q value wa surprising in sense being far away from what wa predicted": [
    "prioritised replay"
  ],
  "current q network w is used select action": [
    "double q learning",
    "dqn improvement"
  ],
  "older q network w is used evaluate action": [
    "double q learning",
    "dqn improvement"
  ],
  "if same weight w used select action evaluate action thi can lead kind confirmation bia": [
    "double q learning"
  ],
  "could maintain two set weight w w one used selection other evaluation then swap their role": [
    "double q learning"
  ],
  "in context deep q learning simpler approach is use current online version w selection an older target version w evaluation we therefore minimize": [
    "double q learning"
  ],
  "rt qw st 1 argmaxb qw st 1 b qw st at 2": [
    "double q learning"
  ],
  "new version w is periodically calculated from distributed value w thi w is broadcast all processor": [
    "double q learning"
  ],
  "action independent value function v": [
    "advantage function"
  ],
  "action dependent advantage function": [
    "advantage function"
  ],
  "q function q can be written sum value function v s plu an advantage function s q s v s": [
    "advantage function"
  ],
  "represent advantage or disadvantage taking action in state s compared taking action preferred current policy we can learn approximation these two component separately": [
    "advantage function"
  ],
  "q vu s aw s": [
    "advantage function"
  ],
  "note that action can be selected just using aw because": [
    "advantage function"
  ],
  "argmaxb q st 1 b argmaxb aw st 1 b": [
    "advantage function"
  ],
  "prioritised replay": [
    "dqn improvement"
  ],
  "double q learning": [
    "dqn improvement"
  ],
  "advantage function s q s v s s": [
    "dqn improvement"
  ],
  "action independent value function vu": [
    "dqn improvement"
  ],
  "action dependent advantage function aw q s vu s aw s": [
    "dqn improvement"
  ],
  "recall fitnes e q log s non episodic game we cannot easily find good estimate q s one approach is consider family q function qw determined parameter w different from learn w so that qw approximate q at same time that policy itself is also being learned thi is known an actor critic approach because policy determine action while q function estimate how good current policy is thereby play role critic": [
    "policy gradient actor critic"
  ],
  "recall that in reinforce algorithm baseline b could be subtracted from rtotal": [
    "advantage actor critic"
  ],
  "rtotal b log at st": [
    "advantage actor critic"
  ],
  "in actor critic framework rtotal is replaced byq st at": [
    "advantage actor critic"
  ],
  "q st at log at st": [
    "advantage actor critic"
  ],
  "we can also subtract baseline from q st at thi baseline must be independent action at but it could be dependent state st good choice baseline is value function vu in which case q function is replaced advantage function": [
    "advantage actor critic"
  ],
  "aw q s vu s": [
    "advantage actor critic"
  ],
  "use policy network choose action": [
    "asynchronou advantage actor critic"
  ],
  "learn parameterized value function vu td learning": [
    "asynchronou advantage actor critic"
  ],
  "estimate q value n step sample": [
    "asynchronou advantage actor critic"
  ],
  "q st at rt 1 rt 2 n 1rt n nvu st n": [
    "asynchronou advantage actor critic"
  ],
  "update policy": [
    "asynchronou advantage actor critic"
  ],
  "q st at vu st log at st update value function my minimizing": [
    "asynchronou advantage actor critic"
  ],
  "q st at vu st 2": [
    "asynchronou advantage actor critic"
  ],
  "learning rate f baseline": [
    "update mean",
    "evolutionary or variational method"
  ],
  "initialize mean i 1 i m standard deviation i 1 i m": [
    "evolutionary or variational method"
  ],
  "each trial collect k sample from gaussian distribution i i i i where i n 0 1": [
    "evolutionary or variational method"
  ],
  "sometime include mirrored sample i i i i": [
    "evolutionary or variational method"
  ],
  "evaluate each sample compute score or fitnes f": [
    "evolutionary or variational method"
  ],
  "update mean f f": [
    "evolutionary or variational method"
  ],
  "sometime is updated well": [
    "evolutionary or variational method"
  ],
  "evolutionary strategy fixed": [
    "openai evolution strategy"
  ],
  "since only is updated computation can be distributed acros manyprocessor": [
    "openai evolution strategy"
  ],
  "applied atarus pong mujoco humanoid walking": [
    "openai evolution strategy"
  ],
  "competitive deep q learning these task": [
    "openai evolution strategy"
  ],
  "select top 20 sample fit new gaussian distribution": [
    "evolutionary strategy",
    "method updating sigma"
  ],
  "minimize reverse kl divergence": [
    "variational inference",
    "method updating sigma"
  ],
  "backpropagate differential through network or differentiate": [
    "variational inference"
  ],
  "variational inference": [
    "method updating sigma"
  ],
  "backpropagate differential through network or differentiate respect i i": [
    "method updating sigma"
  ],
  "t temperature z normalizing constant": [
    "score function f determine boltzmann softmax distribution"
  ],
  "if we only update not thi term is not needed": [
    "t log q can be seen regularizing t erm which maintain some"
  ],
  "kl divergence is used in some policy based deep reinforcement learning algorithm such trust region policy optimization tpro but we will not cover these in detail": [
    "kl divergence"
  ],
  "kl divergence is also important in other area deep learning such variational autoencoder": [
    "kl divergence"
  ],
  "augment a3c unsupervised auxiliary task": [
    "latest research in deep rl"
  ],
  "encourage exploration increased entropy": [
    "latest research in deep rl"
  ],
  "encourage action which reward les predictable": [
    "latest research in deep rl"
  ],
  "concentrate state feature from which preceding action is m orepredictable": [
    "latest research in deep rl"
  ],
  "transfer learning between task": [
    "latest research in deep rl"
  ],
  "inverse reinforcement learning infer reward from policy": [
    "latest research in deep rl"
  ],
  "hierarchical rl": [
    "latest research in deep rl"
  ],
  "multus agent rl": [
    "latest research in deep rl"
  ],
  "content addressable memory": [
    "boltzmann machine outline"
  ],
  "boltzmann machine": [
    "boltzmann machine outline"
  ],
  "restricted boltzmann machine": [
    "boltzmann machine outline"
  ],
  "deep boltzmann machine": [
    "boltzmann machine outline"
  ],
  "greedy layerwise pretraining": [
    "boltzmann machine outline"
  ],
  "human have ability retrieve something from memory when presented only part it example be or not be i came i saw can we recreate thi in computer": [
    "content addressable memory"
  ],
  "we want store set image in neural network in such way that starting corrupted oroccluded version we can recon struct original image": [
    "auto associative memory"
  ],
  "we can try define an energy function e x in configuration space in such way that local minima thi energy function correspond thestored item": [
    "energy based model"
  ],
  "example place n queen an n n chessboard in such way that no two queen attacking each other we assume there is exactly one queen each column so we just need assign row each queen in such way that there no con ict": [
    "constraint satisfaction problem"
  ],
  "some algorithm solving constraint satisfaction problem work iterative improvement or local search these algorithm assign all variable randomly in beginning thu violating several constraint then change one variable at time trying reduce number violation at each step": [
    "local search"
  ],
  "choose value that violate fewest constraint": [
    "value selection min con ict heuristic",
    "hill climbing min conflict"
  ],
  "variable selection randomly select any con icted variable": [
    "hill climbing min conflict"
  ],
  "value selection min con ict heuristic": [
    "hill climbing min conflict"
  ],
  "term hill climbing suggest climbing up region greater fitnes": [
    "hill climbing"
  ],
  "when we minimizing violated constraint it make sense think starting at top ridge climbing down into valley": [
    "inverted view"
  ],
  "number pattern p that can be reliably stored in hopfield network is proportional number neuron d in network": [
    "hopfield network"
  ],
  "careful mathematical analysi show that if p d 0 138 we can expect pattern be stored retrieved successfully": [
    "hopfield network"
  ],
  "if we try store more pattern than these additional spuriou stable state may emerge": [
    "hopfield network"
  ],
  "hopfield network is used store specific item retrieve them what if instead we want generate new item which somehow similar stored item but not quite same thi is known agenerative model first attempt do thi using neural network wa boltzmannmachine": [
    "generative model"
  ],
  "sometime well reproducing training item x i we also want be able use decoder generate new item which similar style training item": [
    "generative model"
  ],
  "in other word we want be able choose latent variable z from standard normal distribution p z feed these value z decoder have it produce new item x which is somehow similar training item": [
    "generative model"
  ],
  "generative model can be": [
    "generative model"
  ],
  "explicit variational autoencoder": [
    "generative model",
    "generative model can be"
  ],
  "implicit generative adversarial network": [
    "generative model",
    "generative model can be"
  ],
  "consider tate x which particular component xi equal 1 suppose we change xto 0 but leave all other component fixed toproduce new state x let e e x e x be difference in energybetween two state thenp x p x etherefore if all other component stay fixed probability xtakingthe value 1 or 0 must bep x 1 p x p x p x 11 ep x 0 1 p x 1 11 e": [
    "gibb sampling"
  ],
  "boltzmann machine is limited in that probability each unit must be linearly separable function surrounding unit it become more powerful if we make division between visible unit v hidden unit h visible hidden unit roughly correspond input hidden unit in feedforward network aim is that hidden unit should learn some hidden feature or latent variable which help system modelthe distribution input": [
    "boltzmann machine limitation"
  ],
  "visible layer v": [
    "two layer bi directional neural network"
  ],
  "hidden layer h": [
    "two layer bi directional neural network"
  ],
  "if we allow visible visible hidden hidden connection thenetwork take too long train so we normally restrict model byallowing only visible hidden connection": [
    "restricted boltzmann machine"
  ],
  "thi is known restricted boltzmann machine": [
    "restricted boltzmann machine"
  ],
  "restricted boltzmann machine we can sample from boltzmann distribution follow choose v randomly then sample hfrom p h v then sample v from p v h then sample h from p h v etc": [
    "alternating gibb sampling"
  ],
  "it wa noticed in early 2000 that proces can be sped up taking just one additional sample instead running many iteration": [
    "quick contrastive divergence"
  ],
  "v0 hare used positive sample v1 h1 ha negative sample": [
    "quick contrastive divergence"
  ],
  "thi can be compared negative sampling that wa used word2vec it is not guaranteed approximate true gradient but it work well in practice": [
    "quick contrastive divergence"
  ],
  "same approach can be applied iteratively multus layer network weight from input first hidden layer trained first keeping those fixed weight from first second hidden layer trained so": [
    "deep boltzmann machine"
  ],
  "one application deep bolzmann machine is greedy unsupervised layerwise pretraining each pair layer in succession is trained an rbm resulting value then used initial weight biase feedforward neural network which is then trained backpropagation some other task such classification sigmoid or tanh activation function thi kind pre training lead much better result than training directly backpropagation from random initial weight": [
    "greedy layerwise pretraining"
  ],
  "autoencoder can be used an alternative restricted bolzmann machine greedy layerwise pretraining": [
    "greedy layerwise pretraining"
  ],
  "an auto encoder one hidden layer is trained reconstruct input first layer encoder thi network become first layer deep network": [
    "greedy layerwise pretraining"
  ],
  "each subsequent layer is then trained reconstruct previouslayer": [
    "greedy layerwise pretraining"
  ],
  "final classification layer is then added resulting deep network whole thing is trained backpropagation": [
    "greedy layerwise pretraining"
  ],
  "autoencoder network 14 1": [
    "autoencoder outline"
  ],
  "regularized autoencoder 14 2": [
    "autoencoder outline"
  ],
  "stochastic encoder decoder 14 4": [
    "autoencoder outline"
  ],
  "variational autoencoder 20 10 3": [
    "autoencoder outline"
  ],
  "after an autoencoder is trained decoder part can be removed replaced example classification layer": [
    "autoencoder pretraining"
  ],
  "thi new network can then be trained backpropagaiton": [
    "autoencoder pretraining"
  ],
  "feature learned autoencoder then serve initial weight supervised learning task": [
    "autoencoder pretraining"
  ],
  "if there more hidden node than input which often happen in image processing there is risk network may learn trivial identity mapping from input output": [
    "avoiding trivial identity"
  ],
  "we generally avoid thi introducing some form regularization": [
    "avoiding trivial identity"
  ],
  "sparse autoencoder": [
    "regularized autoencoder"
  ],
  "autoencoder dropout at hidden layer": [
    "regularized autoencoder"
  ],
  "contractive autoencoder": [
    "regularized autoencoder"
  ],
  "denoising autoencoder": [
    "regularized autoencoder"
  ],
  "one way regularize an autoencoder is add penalty term cost function based hidden unit activation": [
    "sparse autoencoder"
  ],
  "thi is analagou weight decay term we previously used supervised learning": [
    "sparse autoencoder"
  ],
  "one popular choice is penalize sum absolute value activation in hidden layer": [
    "sparse autoencoder"
  ],
  "e l x g f x hi": [
    "sparse autoencoder"
  ],
  "thi is sometime known l regularization because it involf theabsolute value rather than square it can encourage some hidden unit go zero thu producing sparse representation": [
    "sparse autoencoder"
  ],
  "another popular penalty term is l norm derivative thehidden unit respect input": [
    "contractive autoencoder"
  ],
  "e l x g f x x hi 2": [
    "contractive autoencoder"
  ],
  "thi force model learn hidden feature that do not change much when training input x slightly altered": [
    "contractive autoencoder"
  ],
  "squared error assume an underlying gaussian distribution whose": [
    "we saw previously how los cost function at output"
  ],
  "cros entropy assume bernoulli distribution probability": [
    "we saw previously how los cost function at output"
  ],
  "softmax assume boltzmann distribution": [
    "we saw previously how los cost function at output",
    "cost function probability"
  ],
  "we saw previously how los cost function at output feedforward neural network parameter can be seen defining probability distribution p x over output we then train maximize log probability target value": [
    "cost function probability"
  ],
  "squared error assume an underlying gaussian distribution whose mean is output network": [
    "cost function probability"
  ],
  "cros entropy assume bernoulli distribution probability equal output network": [
    "cost function probability"
  ],
  "autoencoder decoder can be seen defining conditional probability distribution p x z output x certain value z hidden or latent variable": [
    "stochastic encoder decoder"
  ],
  "in some case encoder can also be seen defining conditional probability distribution q z x latent variable z based an input x": [
    "stochastic encoder decoder"
  ],
  "we have seen an example thi restricted boltzmannmachine where q z x p x z were bernoulli distribution": [
    "stochastic encoder decoder"
  ],
  "variational autoencoder produce reasonable result": [
    "variational autoencoder"
  ],
  "tend produce blurry image": [
    "variational autoencoder"
  ],
  "often end up using only small number dimension available z": [
    "variational autoencoder"
  ],
  "reference": [
    "variational autoencoder"
  ],
  "http kvfran com variational autoencoder explained": [
    "variational autoencoder"
  ],
  "http cs231n stanford edu slide 2017 cs231n 2017 lecture13 pdf": [
    "variational autoencoder"
  ],
  "http arxiv org pdf 1606 05908 pdfhttp arxiv org pdf 1606 05908 pdf": [
    "variational autoencoder"
  ],
  "artist critic co evolutionco evolution paradigmsblind watchmaker gp artist human critic evolutonary art gp artist gp or nn critic generative adversarial network cnn artist cnn critic": [
    "adversarial training gan outline"
  ],
  "output is trained reproduce input closely possible": [
    "autoencoder network"
  ],
  "activation normally pas through bottleneck so network is forced compres datum in some way": [
    "autoencoder network"
  ],
  "like rbm auto encoder can be used automatically extract abstract feature from input": [
    "autoencoder network"
  ],
  "if encoder compute z f x decoder compute g f x then we aim minimize some distance function between x g f x": [
    "autoencoder network"
  ],
  "e l x g f x": [
    "autoencoder network"
  ],
  "normalize image between 1 1": [
    "gan convolutional architecture"
  ],
  "replace pooling layer": [
    "gan convolutional architecture"
  ],
  "strided convolution discriminator": [
    "gan convolutional architecture"
  ],
  "fractional strided convolution generator": [
    "gan convolutional architecture"
  ],
  "use batchnorm in both generator discriminator": [
    "gan convolutional architecture"
  ],
  "remove fully connected hidden layer deeper architecture": [
    "gan convolutional architecture"
  ],
  "use tanh at output layer generator relu activation in all other layer": [
    "gan convolutional architecture"
  ],
  "use leakyrelu activation all layer discriminator": [
    "gan convolutional architecture"
  ],
  "unsupervised representation learning deep convolutional generative adversarial network radford et al 2016": [
    "generator architecture"
  ],
  "like any coevolution gan can sometime oscillate or getstuck in mediocre stable state image but quality fail improve compare ipd desired range image or converge single image minor variation method avoiding mode collapse conditioning augmentationminibatch feature fitnes sharing unrolled gan": [
    "mode collapse generator produce only small subset"
  ],
  "like any coevolution gan can sometime oscillate or get stuck in mediocre stable state": [
    "mediocre stable state"
  ],
  "oscillation gan train long time generating variety image but quality fail improve compare ipd": [
    "mediocre stable state"
  ],
  "mode collapse generator produce only small subset desired range image or converge single image minor variation": [
    "mediocre stable state"
  ],
  "method avoiding mode collapse": [
    "mediocre stable state"
  ],
  "conditioning augmentation": [
    "mediocre stable state",
    "mediocre stable state",
    "method avoiding mode collapse"
  ],
  "minibatch feature fitnes sharing": [
    "mediocre stable state",
    "mediocre stable state",
    "method avoiding mode collapse"
  ],
  "unrolled gan": [
    "mediocre stable state",
    "mediocre stable state",
    "method avoiding mode collapse"
  ],
  "contex encoder image inpainting": [
    "gan zoo"
  ],
  "texture synthesi patch based gan": [
    "gan zoo"
  ],
  "conditional gan": [
    "gan zoo"
  ],
  "text image synthesi": [
    "gan zoo"
  ],
  "stackgan": [
    "gan zoo"
  ],
  "patch based discriminator": [
    "gan zoo"
  ],
  "s2 gan": [
    "gan zoo"
  ],
  "style gan": [
    "gan zoo"
  ],
  "plug play generative network": [
    "gan zoo"
  ],
  "http dl ee cuhk edu hk slide gan pdf": [
    "gan reference"
  ],
  "cs231n stanford edu slide 2017 cs231n 2017 lecture13 pdf": [
    "gan reference"
  ],
  "http www iangoodfellow com slide 2016 12 04 nip pdf": [
    "gan reference"
  ],
  "http arxiv org ab 1612 00005": [
    "gan reference"
  ],
  "evolutionary computation paradigm": [
    "coevolution outline"
  ],
  "deceptive landscape": [
    "coevolution outline"
  ],
  "punctuated equilibrium": [
    "coevolution outline",
    "eldridge gould 1970"
  ],
  "coevolution in nature": [
    "coevolution outline"
  ],
  "coevolution in machine learning": [
    "coevolution outline"
  ],
  "evaluation": [
    "repeated cycle",
    "evolutionary computation",
    "evolutionary computation"
  ],
  "selection": [
    "repeated cycle",
    "evolutionary computation",
    "evolutionary computation"
  ],
  "reproduction mutation": [
    "repeated cycle",
    "evolutionary computation",
    "evolutionary computation"
  ],
  "use principle natural selection evolve computational mechanism which perform well at specified task": [
    "evolutionary computation"
  ],
  "start randomly initialized population": [
    "evolutionary computation"
  ],
  "repeated cycle": [
    "evolutionary computation"
  ],
  "any computational paradigm can be used appropriately defined reproduction mutation operator": [
    "evolutionary computation"
  ],
  "let assume we have population 100 individual": [
    "evolutionary computation"
  ],
  "at each generation we evaluate fitnes score each individual insome case thi may require tranlating from genotype phenotype": [
    "evolutionary computation"
  ],
  "best 50 individual selected other 50 culled orremoved from population": [
    "evolutionary computation"
  ],
  "crossover mutation operator applied selected individual producing 50 new individual replace those who were culled": [
    "evolutionary computation"
  ],
  "we then evaluate new population 100 individual cyclerepeat": [
    "evolutionary computation"
  ],
  "representation": [
    "bit string crossover",
    "human vs machine",
    "evolutionary issue"
  ],
  "mutation operator": [
    "bit string crossover",
    "human vs machine",
    "evolutionary issue"
  ],
  "crossover operator": [
    "bit string crossover",
    "human vs machine",
    "evolutionary issue"
  ],
  "fitnes function": [
    "bit string crossover",
    "human vs machine",
    "evolutionary issue"
  ],
  "continuou parameter swefel evolutionary strategy": [
    "expression tree",
    "representation"
  ],
  "bit string holland genetic algorithm": [
    "expression tree",
    "representation"
  ],
  "expression tree koza genetic programming": [
    "expression tree",
    "representation"
  ],
  "lindenmeyer system e g sim evolving virtual creature": [
    "expression tree",
    "representation"
  ],
  "implicit parallelism": [
    "schema theorem"
  ],
  "fitter schema increase their representation over time": [
    "schema theorem"
  ],
  "schema combine like building block": [
    "schema theorem"
  ],
  "sometime fitnes function present asooth hill algorithm climb but often we see deceptive landscape leading premature convergence where population get stuck local opmi mum": [
    "fitnes function"
  ],
  "fitnes sharing": [
    "fitnes function"
  ],
  "random re start": [
    "fitnes function"
  ],
  "age layered plane alp": [
    "fitnes function"
  ],
  "spatial coevolution": [
    "fitnes function"
  ],
  "learning done actual robot": [
    "aibo walk learning"
  ],
  "learning done in simulator then tested actual robot": [
    "guroo humanoid walk learning"
  ],
  "one example use evolu tionary algorithm real world application is antenna that wa evolved hornby et al in 2006 nasa space technology 5 st5 mission": [
    "evolved antenna"
  ],
  "partial geographic isolation": [
    "eldridge gould 1970"
  ],
  "island model": [
    "idea evolutionary computation"
  ],
  "co evolution artificial ecology": [
    "idea evolutionary computation"
  ],
  "eldridge gould 1970": [
    "gap in fossil record"
  ],
  "idea evolutionary computation": [
    "gap in fossil record"
  ],
  "competitive leopard vs gazelle": [
    "co evolution in nature"
  ],
  "co operative insect ower": [
    "co evolution in nature"
  ],
  "mixed co operative competitive maynard smith": [
    "co evolution in nature"
  ],
  "different gene within same genome": [
    "co evolution in nature"
  ],
  "diffuse co evolution": [
    "co evolution in nature"
  ],
  "brain body sim lipson": [
    "co evolution in machine learning"
  ],
  "strategic game backgammon ches go": [
    "co evolution in machine learning"
  ],
  "human vs machine tron": [
    "co evolution in machine learning"
  ],
  "algorithm vs test case hilli": [
    "co evolution in machine learning"
  ],
  "mixed co operative competitive ipd": [
    "co evolution in machine learning"
  ],
  "language game tonke ficicus": [
    "co evolution in machine learning"
  ],
  "artist critic evolutionary art": [
    "co evolution in machine learning"
  ],
  "body evolf lindenmeyer system": [
    "evolving virtual creature"
  ],
  "controller evolf neural network": [
    "evolving virtual creature"
  ],
  "evolved in simulation tested in reality lipson": [
    "golem"
  ],
  "two co evolving population network string": [
    "sorting network",
    "sorting network 2"
  ],
  "can escape from local optima": [
    "sorting network",
    "sorting network 2"
  ],
  "punctuated equilibrium observed": [
    "sorting network",
    "sorting network 2"
  ],
  "better than hand crafted solution tuft juille pollack": [
    "sorting network",
    "sorting network 2"
  ],
  "evolving population network": [
    "sorting network 1"
  ],
  "converged local optimum": [
    "sorting network 1"
  ],
  "final network not quite good hand crafted human solution": [
    "sorting network 1"
  ],
  "co evolution tend provide an opponent appropriate ability": [
    "metum game learning"
  ],
  "generally help escape from local optima": [
    "metum game learning"
  ],
  "however can create new mediocre stable state collusion": [
    "metum game learning"
  ],
  "artist genetic program gp or hercl": [
    "evolutionary art"
  ],
  "artist used function compute r g b value each pixel location x y": [
    "evolutionary art"
  ],
  "alternatively artist issue series drawing instruction": [
    "evolutionary art"
  ],
  "critic gp evolution or neural network backpropagation": [
    "evolutionary art"
  ],
  "critic is presented real image from training set fake image generated artist": [
    "evolutionary art"
  ],
  "critic is trained produce output close 1 real image close 0 generated image or vice versa": [
    "evolutionary art"
  ],
  "input critic": [
    "evolutionary art"
  ],
  "small number statistical feature extracted from image": [
    "evolutionary art"
  ],
  "artist hercl program function from x y r g b": [
    "computational creativity"
  ],
  "critic deep convolutional neural network lenet": [
    "computational creativity"
  ],
  "1 neuroanatomy2 perceptrons3 backpropagation4 variation backprop5 geometry6 convolutional networks7 image processing8 recurrent networks9 language processing10 reinforcement learning11 deep reinforcement learning12 boltzmann machines13 autoencoders14 adversarial training gans15 coevolution16 evolutionary art": [
    "topic covered"
  ],
  "neuroanatomy lect 1": [
    "not examinable"
  ],
  "coevolution lect 15": [
    "not examinable"
  ],
  "evolutionary art lect 16": [
    "not examinable"
  ],
  "2 hmy exam centrally managed": [
    "final exam"
  ],
  "worth 60 final mark": [
    "final exam"
  ],
  "no textbook or course note": [
    "final exam"
  ],
  "approved calculator may be used": [
    "final exam"
  ],
  "exercise quiz course web site give good indicationof kind question expect in exam": [
    "final exam"
  ],
  "sample exam now available c ourse web site": [
    "final exam"
  ],
  "deep learning combined evolution": [
    "possible 4th year project"
  ],
  "deep learning signal processing": [
    "possible 4th year project"
  ],
  "self normalizing activation function": [
    "possible 4th year project"
  ],
  "cnn internal weight symmetry": [
    "possible 4th year project"
  ],
  "adversarial coevolution hercl program": [
    "possible 4th year project"
  ],
  "other topic in deep learning evolution game": [
    "possible 4th year project"
  ],
  "please remember fill in unsw myexperience survey": [
    "unsw myexperience survey"
  ]
}

